
==> Audit <==
|-----------|------------------|----------|--------------------|---------|---------------------|---------------------|
|  Command  |       Args       | Profile  |        User        | Version |     Start Time      |      End Time       |
|-----------|------------------|----------|--------------------|---------|---------------------|---------------------|
| start     |                  | minikube | mariafernandalopes | v1.34.0 | 25 Sep 24 08:50 -03 |                     |
| start     | --driver=docker  | minikube | mariafernandalopes | v1.34.0 | 25 Sep 24 08:50 -03 |                     |
| dashboard |                  | minikube | mariafernandalopes | v1.34.0 | 25 Sep 24 08:51 -03 |                     |
| start     |                  | minikube | mariafernandalopes | v1.34.0 | 26 Oct 24 21:20 -03 |                     |
| start     |                  | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 15:57 -03 | 29 Oct 24 16:01 -03 |
| service   | airports-service | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 21:21 -03 |                     |
| service   | flights-service  | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 21:21 -03 |                     |
| service   | users-service    | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 21:21 -03 |                     |
| service   | airports-service | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 21:22 -03 |                     |
| ssh       |                  | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 21:25 -03 | 29 Oct 24 21:25 -03 |
| service   | airports-service | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 21:30 -03 |                     |
| start     |                  | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 21:32 -03 | 29 Oct 24 21:35 -03 |
| service   | airports-service | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 21:36 -03 |                     |
| service   | airports-service | minikube | mariafernandalopes | v1.34.0 | 29 Oct 24 21:43 -03 |                     |
|-----------|------------------|----------|--------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/10/29 21:32:04
Running on machine: Maf
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1029 21:32:04.937736   78168 out.go:345] Setting OutFile to fd 1 ...
I1029 21:32:04.938510   78168 out.go:358] Setting ErrFile to fd 2...
I1029 21:32:04.938644   78168 root.go:338] Updating PATH: /home/mariafernandalopes/.minikube/bin
W1029 21:32:04.939267   78168 root.go:314] Error reading config file at /home/mariafernandalopes/.minikube/config/config.json: open /home/mariafernandalopes/.minikube/config/config.json: no such file or directory
I1029 21:32:04.944796   78168 out.go:352] Setting JSON to false
I1029 21:32:04.957755   78168 start.go:129] hostinfo: {"hostname":"Maf","uptime":82153,"bootTime":1730166172,"procs":93,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.153.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"a09fbdb4-08e8-4f2a-af0e-1626f50b89b2"}
I1029 21:32:04.957867   78168 start.go:139] virtualization:  guest
I1029 21:32:04.963266   78168 out.go:177] üòÑ  minikube v1.34.0 on Ubuntu 22.04 (amd64)
I1029 21:32:04.968270   78168 notify.go:220] Checking for updates...
I1029 21:32:04.968427   78168 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1029 21:32:04.972095   78168 driver.go:394] Setting default libvirt URI to qemu:///system
I1029 21:32:05.494629   78168 docker.go:123] docker version: linux-27.1.1:Docker Desktop  ()
I1029 21:32:05.497046   78168 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1029 21:32:23.080579   78168 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (17.595872319s)
I1029 21:32:23.140076   78168 info.go:266] docker info: {ID:3c366b1e-bf26-460f-9513-d59962e1d57f Containers:15 ContainersRunning:3 ContainersPaused:0 ContainersStopped:12 Images:17 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:82 OomKillDisable:true NGoroutines:90 SystemTime:2024-10-30 00:32:22.595766318 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3835625472 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I1029 21:32:23.148880   78168 docker.go:318] overlay module found
I1029 21:32:23.168999   78168 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1029 21:32:23.188569   78168 start.go:297] selected driver: docker
I1029 21:32:23.189048   78168 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mariafernandalopes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1029 21:32:23.201843   78168 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1029 21:32:23.216891   78168 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1029 21:32:24.621607   78168 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.403194693s)
I1029 21:32:24.624675   78168 info.go:266] docker info: {ID:3c366b1e-bf26-460f-9513-d59962e1d57f Containers:15 ContainersRunning:3 ContainersPaused:0 ContainersStopped:12 Images:17 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:82 OomKillDisable:true NGoroutines:90 SystemTime:2024-10-30 00:32:24.531385252 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3835625472 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I1029 21:32:24.671258   78168 cni.go:84] Creating CNI manager for ""
I1029 21:32:24.672452   78168 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1029 21:32:24.675470   78168 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mariafernandalopes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1029 21:32:24.682755   78168 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1029 21:32:24.689525   78168 cache.go:121] Beginning downloading kic base image for docker with docker
I1029 21:32:24.697091   78168 out.go:177] üöú  Pulling base image v0.0.45 ...
I1029 21:32:24.703781   78168 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1029 21:32:24.705104   78168 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1029 21:32:24.705896   78168 preload.go:146] Found local preload: /home/mariafernandalopes/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1029 21:32:24.708106   78168 cache.go:56] Caching tarball of preloaded images
I1029 21:32:24.720228   78168 preload.go:172] Found /home/mariafernandalopes/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1029 21:32:24.720508   78168 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1029 21:32:24.729063   78168 profile.go:143] Saving config to /home/mariafernandalopes/.minikube/profiles/minikube/config.json ...
W1029 21:32:25.278074   78168 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1029 21:32:25.278197   78168 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1029 21:32:25.281026   78168 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1029 21:32:25.286470   78168 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1029 21:32:25.286758   78168 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1029 21:32:25.286802   78168 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1029 21:32:25.286954   78168 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1029 21:32:41.961037   78168 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1029 21:32:41.963016   78168 cache.go:194] Successfully downloaded all kic artifacts
I1029 21:32:41.968651   78168 start.go:360] acquireMachinesLock for minikube: {Name:mk53794888d1396285e73e700ff52b1c5300db92 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1029 21:32:41.977792   78168 start.go:364] duration metric: took 8.354588ms to acquireMachinesLock for "minikube"
I1029 21:32:41.978180   78168 start.go:96] Skipping create...Using existing machine configuration
I1029 21:32:41.979207   78168 fix.go:54] fixHost starting: 
I1029 21:32:42.003876   78168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1029 21:32:42.811601   78168 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W1029 21:32:42.813399   78168 fix.go:138] unexpected machine state, will restart: <nil>
I1029 21:32:42.829175   78168 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I1029 21:32:42.835369   78168 machine.go:93] provisionDockerMachine start ...
I1029 21:32:42.837791   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:43.111065   78168 main.go:141] libmachine: Using SSH client type: native
I1029 21:32:43.124289   78168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 49968 <nil> <nil>}
I1029 21:32:43.124387   78168 main.go:141] libmachine: About to run SSH command:
hostname
I1029 21:32:45.736782   78168 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1029 21:32:45.739385   78168 ubuntu.go:169] provisioning hostname "minikube"
I1029 21:32:45.742649   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:47.342577   78168 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.598489959s)
I1029 21:32:47.352178   78168 main.go:141] libmachine: Using SSH client type: native
I1029 21:32:47.354806   78168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 49968 <nil> <nil>}
I1029 21:32:47.354836   78168 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1029 21:32:50.714753   78168 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1029 21:32:50.717724   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:51.427507   78168 main.go:141] libmachine: Using SSH client type: native
I1029 21:32:51.428835   78168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 49968 <nil> <nil>}
I1029 21:32:51.428865   78168 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1029 21:32:51.802825   78168 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1029 21:32:51.805808   78168 ubuntu.go:175] set auth options {CertDir:/home/mariafernandalopes/.minikube CaCertPath:/home/mariafernandalopes/.minikube/certs/ca.pem CaPrivateKeyPath:/home/mariafernandalopes/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/mariafernandalopes/.minikube/machines/server.pem ServerKeyPath:/home/mariafernandalopes/.minikube/machines/server-key.pem ClientKeyPath:/home/mariafernandalopes/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/mariafernandalopes/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/mariafernandalopes/.minikube}
I1029 21:32:51.805930   78168 ubuntu.go:177] setting up certificates
I1029 21:32:51.806971   78168 provision.go:84] configureAuth start
I1029 21:32:51.810309   78168 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1029 21:32:52.123918   78168 provision.go:143] copyHostCerts
I1029 21:32:52.126525   78168 exec_runner.go:144] found /home/mariafernandalopes/.minikube/ca.pem, removing ...
I1029 21:32:52.126801   78168 exec_runner.go:203] rm: /home/mariafernandalopes/.minikube/ca.pem
I1029 21:32:52.127826   78168 exec_runner.go:151] cp: /home/mariafernandalopes/.minikube/certs/ca.pem --> /home/mariafernandalopes/.minikube/ca.pem (1111 bytes)
I1029 21:32:52.129051   78168 exec_runner.go:144] found /home/mariafernandalopes/.minikube/cert.pem, removing ...
I1029 21:32:52.129057   78168 exec_runner.go:203] rm: /home/mariafernandalopes/.minikube/cert.pem
I1029 21:32:52.129090   78168 exec_runner.go:151] cp: /home/mariafernandalopes/.minikube/certs/cert.pem --> /home/mariafernandalopes/.minikube/cert.pem (1151 bytes)
I1029 21:32:52.130614   78168 exec_runner.go:144] found /home/mariafernandalopes/.minikube/key.pem, removing ...
I1029 21:32:52.130618   78168 exec_runner.go:203] rm: /home/mariafernandalopes/.minikube/key.pem
I1029 21:32:52.130646   78168 exec_runner.go:151] cp: /home/mariafernandalopes/.minikube/certs/key.pem --> /home/mariafernandalopes/.minikube/key.pem (1675 bytes)
I1029 21:32:52.132786   78168 provision.go:117] generating server cert: /home/mariafernandalopes/.minikube/machines/server.pem ca-key=/home/mariafernandalopes/.minikube/certs/ca.pem private-key=/home/mariafernandalopes/.minikube/certs/ca-key.pem org=mariafernandalopes.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1029 21:32:52.816356   78168 provision.go:177] copyRemoteCerts
I1029 21:32:52.819098   78168 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1029 21:32:52.819311   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:53.092698   78168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49968 SSHKeyPath:/home/mariafernandalopes/.minikube/machines/minikube/id_rsa Username:docker}
I1029 21:32:53.288918   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I1029 21:32:53.381376   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1029 21:32:53.569573   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1111 bytes)
I1029 21:32:53.738603   78168 provision.go:87] duration metric: took 1.930926324s to configureAuth
I1029 21:32:53.738639   78168 ubuntu.go:193] setting minikube options for container-runtime
I1029 21:32:53.740194   78168 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1029 21:32:53.740405   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:53.867905   78168 main.go:141] libmachine: Using SSH client type: native
I1029 21:32:53.868117   78168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 49968 <nil> <nil>}
I1029 21:32:53.868135   78168 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1029 21:32:54.124873   78168 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1029 21:32:54.124898   78168 ubuntu.go:71] root file system type: overlay
I1029 21:32:54.126462   78168 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1029 21:32:54.126783   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:54.209410   78168 main.go:141] libmachine: Using SSH client type: native
I1029 21:32:54.209876   78168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 49968 <nil> <nil>}
I1029 21:32:54.209932   78168 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1029 21:32:54.483729   78168 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1029 21:32:54.483969   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:54.624470   78168 main.go:141] libmachine: Using SSH client type: native
I1029 21:32:54.624718   78168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 49968 <nil> <nil>}
I1029 21:32:54.624731   78168 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1029 21:32:54.834284   78168 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1029 21:32:54.834567   78168 machine.go:96] duration metric: took 11.985743066s to provisionDockerMachine
I1029 21:32:54.835050   78168 start.go:293] postStartSetup for "minikube" (driver="docker")
I1029 21:32:54.835430   78168 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1029 21:32:54.835594   78168 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1029 21:32:54.835829   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:55.078894   78168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49968 SSHKeyPath:/home/mariafernandalopes/.minikube/machines/minikube/id_rsa Username:docker}
I1029 21:32:55.225484   78168 ssh_runner.go:195] Run: cat /etc/os-release
I1029 21:32:55.236237   78168 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1029 21:32:55.236256   78168 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1029 21:32:55.236262   78168 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1029 21:32:55.236278   78168 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1029 21:32:55.236524   78168 filesync.go:126] Scanning /home/mariafernandalopes/.minikube/addons for local assets ...
I1029 21:32:55.239352   78168 filesync.go:126] Scanning /home/mariafernandalopes/.minikube/files for local assets ...
I1029 21:32:55.240029   78168 start.go:296] duration metric: took 404.680694ms for postStartSetup
I1029 21:32:55.240651   78168 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1029 21:32:55.240764   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:55.426445   78168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49968 SSHKeyPath:/home/mariafernandalopes/.minikube/machines/minikube/id_rsa Username:docker}
I1029 21:32:55.594255   78168 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1029 21:32:55.607137   78168 fix.go:56] duration metric: took 13.615677302s for fixHost
I1029 21:32:55.607463   78168 start.go:83] releasing machines lock for "minikube", held for 13.616385953s
I1029 21:32:55.608233   78168 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1029 21:32:55.761569   78168 ssh_runner.go:195] Run: cat /version.json
I1029 21:32:55.761802   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:55.762105   78168 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1029 21:32:55.763542   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:32:55.901002   78168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49968 SSHKeyPath:/home/mariafernandalopes/.minikube/machines/minikube/id_rsa Username:docker}
I1029 21:32:55.935468   78168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49968 SSHKeyPath:/home/mariafernandalopes/.minikube/machines/minikube/id_rsa Username:docker}
I1029 21:32:56.105741   78168 ssh_runner.go:195] Run: systemctl --version
I1029 21:32:56.919610   78168 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.157485012s)
I1029 21:32:56.920724   78168 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1029 21:32:56.932252   78168 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1029 21:32:56.988787   78168 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1029 21:32:57.018812   78168 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1029 21:32:57.126381   78168 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1029 21:32:57.126764   78168 start.go:495] detecting cgroup driver to use...
I1029 21:32:57.126911   78168 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1029 21:32:57.131203   78168 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1029 21:32:57.173154   78168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1029 21:32:57.211167   78168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1029 21:32:57.251528   78168 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1029 21:32:57.252885   78168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1029 21:32:57.296845   78168 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1029 21:32:57.321662   78168 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1029 21:32:57.339920   78168 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1029 21:32:57.361686   78168 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1029 21:32:57.382827   78168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1029 21:32:57.413717   78168 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1029 21:32:57.435556   78168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1029 21:32:57.478916   78168 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1029 21:32:57.492859   78168 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1029 21:32:57.505488   78168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 21:32:58.396939   78168 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1029 21:33:14.582385   78168 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (16.183298194s)
I1029 21:33:14.586957   78168 start.go:495] detecting cgroup driver to use...
I1029 21:33:14.590544   78168 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1029 21:33:14.594141   78168 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1029 21:33:14.707143   78168 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1029 21:33:14.708409   78168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1029 21:33:14.772824   78168 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1029 21:33:14.848399   78168 ssh_runner.go:195] Run: which cri-dockerd
I1029 21:33:14.862620   78168 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1029 21:33:14.931819   78168 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1029 21:33:15.058102   78168 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1029 21:33:15.553062   78168 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1029 21:33:15.846265   78168 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1029 21:33:15.873187   78168 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1029 21:33:15.984546   78168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 21:33:16.618614   78168 ssh_runner.go:195] Run: sudo systemctl restart docker
I1029 21:33:20.277865   78168 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.658182981s)
I1029 21:33:20.282549   78168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1029 21:33:20.327222   78168 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1029 21:33:20.413119   78168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1029 21:33:20.432780   78168 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1029 21:33:20.658098   78168 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1029 21:33:20.792821   78168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 21:33:20.961413   78168 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1029 21:33:20.983845   78168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1029 21:33:21.022392   78168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 21:33:21.222013   78168 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1029 21:33:26.862855   78168 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker.service: (5.809281565s)
I1029 21:33:26.863699   78168 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1029 21:33:26.870686   78168 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1029 21:33:27.037099   78168 start.go:563] Will wait 60s for crictl version
I1029 21:33:27.038307   78168 ssh_runner.go:195] Run: which crictl
I1029 21:33:29.562698   78168 ssh_runner.go:235] Completed: which crictl: (2.523441425s)
I1029 21:33:29.566756   78168 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1029 21:33:54.354454   78168 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (24.787006242s)
I1029 21:33:54.357159   78168 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1029 21:33:54.366527   78168 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1029 21:34:18.147897   78168 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (24.365055197s)
I1029 21:34:18.155067   78168 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1029 21:34:30.305800   78168 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (12.388211353s)
I1029 21:34:30.340472   78168 out.go:235] üê≥  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1029 21:34:30.370037   78168 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1029 21:34:32.924802   78168 cli_runner.go:217] Completed: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}": (2.551967077s)
I1029 21:34:32.953565   78168 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1029 21:34:33.321653   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1029 21:34:33.822577   78168 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mariafernandalopes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1029 21:34:33.850029   78168 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1029 21:34:33.852713   78168 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1029 21:34:37.203251   78168 ssh_runner.go:235] Completed: docker images --format {{.Repository}}:{{.Tag}}: (3.349819935s)
I1029 21:34:37.207459   78168 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1029 21:34:37.207856   78168 docker.go:615] Images already preloaded, skipping extraction
I1029 21:34:37.216088   78168 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1029 21:34:37.728881   78168 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1029 21:34:37.733936   78168 cache_images.go:84] Images are preloaded, skipping loading
I1029 21:34:37.736181   78168 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1029 21:34:37.753205   78168 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1029 21:34:37.755077   78168 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1029 21:34:40.827484   78168 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (3.071571374s)
I1029 21:34:41.081939   78168 cni.go:84] Creating CNI manager for ""
I1029 21:34:41.082703   78168 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1029 21:34:41.083550   78168 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1029 21:34:41.088987   78168 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1029 21:34:41.129479   78168 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1029 21:34:41.138661   78168 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1029 21:34:42.414495   78168 ssh_runner.go:235] Completed: sudo ls /var/lib/minikube/binaries/v1.31.0: (1.275714408s)
I1029 21:34:42.415090   78168 binaries.go:44] Found k8s binaries, skipping transfer
I1029 21:34:42.416514   78168 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1029 21:34:42.627274   78168 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1029 21:34:44.515011   78168 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1029 21:34:45.224414   78168 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1029 21:34:45.726274   78168 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1029 21:34:45.766722   78168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 21:34:48.148560   78168 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (2.381241267s)
I1029 21:34:48.150152   78168 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1029 21:34:48.619949   78168 certs.go:68] Setting up /home/mariafernandalopes/.minikube/profiles/minikube for IP: 192.168.49.2
I1029 21:34:48.620097   78168 certs.go:194] generating shared ca certs ...
I1029 21:34:48.644814   78168 certs.go:226] acquiring lock for ca certs: {Name:mk7888987e91eeed2bb836cfd50578bdd67c56f5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1029 21:34:48.965671   78168 certs.go:235] skipping valid "minikubeCA" ca cert: /home/mariafernandalopes/.minikube/ca.key
I1029 21:34:49.070798   78168 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/mariafernandalopes/.minikube/proxy-client-ca.key
I1029 21:34:49.071776   78168 certs.go:256] generating profile certs ...
I1029 21:34:49.088956   78168 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/mariafernandalopes/.minikube/profiles/minikube/client.key
I1029 21:34:49.095009   78168 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/mariafernandalopes/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1029 21:34:49.100245   78168 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/mariafernandalopes/.minikube/profiles/minikube/proxy-client.key
I1029 21:34:49.106441   78168 certs.go:484] found cert: /home/mariafernandalopes/.minikube/certs/ca-key.pem (1679 bytes)
I1029 21:34:49.119533   78168 certs.go:484] found cert: /home/mariafernandalopes/.minikube/certs/ca.pem (1111 bytes)
I1029 21:34:49.119707   78168 certs.go:484] found cert: /home/mariafernandalopes/.minikube/certs/cert.pem (1151 bytes)
I1029 21:34:49.120870   78168 certs.go:484] found cert: /home/mariafernandalopes/.minikube/certs/key.pem (1675 bytes)
I1029 21:34:50.406734   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1029 21:34:53.520046   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1029 21:35:00.097729   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1029 21:35:01.398362   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1029 21:35:09.788228   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1029 21:35:10.574197   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1029 21:35:11.083781   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1029 21:35:11.569851   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1029 21:35:11.876069   78168 ssh_runner.go:362] scp /home/mariafernandalopes/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1029 21:35:15.292835   78168 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1029 21:35:19.488643   78168 ssh_runner.go:195] Run: openssl version
I1029 21:35:19.712277   78168 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1029 21:35:19.830249   78168 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1029 21:35:19.844782   78168 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 29 19:01 /usr/share/ca-certificates/minikubeCA.pem
I1029 21:35:19.846024   78168 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1029 21:35:19.943661   78168 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1029 21:35:19.995324   78168 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1029 21:35:20.027641   78168 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1029 21:35:20.063597   78168 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1029 21:35:20.195669   78168 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1029 21:35:23.129329   78168 ssh_runner.go:235] Completed: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400: (2.932860054s)
I1029 21:35:23.133445   78168 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1029 21:35:23.508705   78168 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1029 21:35:23.826145   78168 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1029 21:35:23.923262   78168 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mariafernandalopes:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1029 21:35:23.988222   78168 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1029 21:35:26.697386   78168 ssh_runner.go:235] Completed: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}: (2.707874341s)
I1029 21:35:26.701845   78168 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1029 21:35:26.983199   78168 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1029 21:35:26.984784   78168 kubeadm.go:593] restartPrimaryControlPlane start ...
I1029 21:35:26.985430   78168 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1029 21:35:27.045268   78168 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1029 21:35:27.050673   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1029 21:35:29.819787   78168 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube: (2.830105647s)
I1029 21:35:29.918432   78168 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:49967"
I1029 21:35:30.124951   78168 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1029 21:35:30.245657   78168 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1029 21:35:30.248176   78168 kubeadm.go:597] duration metric: took 3.325879747s to restartPrimaryControlPlane
I1029 21:35:30.248288   78168 kubeadm.go:394] duration metric: took 6.39110574s to StartCluster
I1029 21:35:30.248946   78168 settings.go:142] acquiring lock: {Name:mkd0b9b778ac288d6f79f1ffd4a93321f9940253 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1029 21:35:30.251641   78168 settings.go:150] Updating kubeconfig:  /home/mariafernandalopes/.kube/config
I1029 21:35:30.288599   78168 lock.go:35] WriteFile acquiring /home/mariafernandalopes/.kube/config: {Name:mk9559671bc184c14ecb6abfbc5b30c17b5110c0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1029 21:35:30.302948   78168 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1029 21:35:30.303670   78168 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1029 21:35:30.304784   78168 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1029 21:35:30.304836   78168 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1029 21:35:30.306845   78168 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1029 21:35:30.306884   78168 addons.go:243] addon storage-provisioner should already be in state true
I1029 21:35:30.307498   78168 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1029 21:35:30.312828   78168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1029 21:35:30.352129   78168 host.go:66] Checking if "minikube" exists ...
I1029 21:35:30.353618   78168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1029 21:35:30.382277   78168 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1029 21:35:30.396344   78168 out.go:177] üîé  Verifying Kubernetes components...
I1029 21:35:30.443557   78168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1029 21:35:31.177688   78168 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1029 21:35:31.211286   78168 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1029 21:35:31.212257   78168 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1029 21:35:31.213886   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:35:31.385306   78168 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.068057129s)
I1029 21:35:31.455146   78168 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1029 21:35:31.455326   78168 addons.go:243] addon default-storageclass should already be in state true
I1029 21:35:31.457593   78168 host.go:66] Checking if "minikube" exists ...
I1029 21:35:31.461514   78168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1029 21:35:32.491593   78168 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.276355955s)
I1029 21:35:32.495370   78168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49968 SSHKeyPath:/home/mariafernandalopes/.minikube/machines/minikube/id_rsa Username:docker}
I1029 21:35:32.500974   78168 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.03752119s)
I1029 21:35:32.503735   78168 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1029 21:35:32.504437   78168 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1029 21:35:32.507929   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1029 21:35:32.756891   78168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49968 SSHKeyPath:/home/mariafernandalopes/.minikube/machines/minikube/id_rsa Username:docker}
I1029 21:35:33.607924   78168 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (3.1641869s)
I1029 21:35:33.608948   78168 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1029 21:35:34.114089   78168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1029 21:35:35.062737   78168 api_server.go:52] waiting for apiserver process to appear ...
I1029 21:35:35.065563   78168 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1029 21:35:35.207291   78168 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1029 21:35:35.220910   78168 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1029 21:35:35.607778   78168 api_server.go:72] duration metric: took 5.303749871s to wait for apiserver process to appear ...
I1029 21:35:35.608141   78168 api_server.go:88] waiting for apiserver healthz status ...
I1029 21:35:35.608563   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:40.127961   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1029 21:35:40.129411   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1029 21:35:40.129731   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:40.310405   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1029 21:35:40.310547   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1029 21:35:40.609113   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:40.632377   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:40.632617   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:41.108678   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:41.134279   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:41.134402   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:41.609249   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:41.720094   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:41.720696   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:42.108635   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:42.109297   78168 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (6.901595588s)
I1029 21:35:42.128331   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:42.128427   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:42.609152   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:42.621597   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:42.621835   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:43.108644   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:43.120463   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:43.120514   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:43.616331   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:44.109713   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:44.110068   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:44.110218   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:44.218964   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:44.219067   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:44.609243   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:44.705425   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:44.706090   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:45.109160   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:45.122005   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1029 21:35:45.122049   78168 api_server.go:103] status: https://127.0.0.1:49967/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1029 21:35:45.609516   78168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49967/healthz ...
I1029 21:35:45.621505   78168 api_server.go:279] https://127.0.0.1:49967/healthz returned 200:
ok
I1029 21:35:45.817996   78168 api_server.go:141] control plane version: v1.31.0
I1029 21:35:45.818509   78168 api_server.go:131] duration metric: took 10.210152987s to wait for apiserver health ...
I1029 21:35:45.820758   78168 system_pods.go:43] waiting for kube-system pods to appear ...
I1029 21:35:46.075911   78168 system_pods.go:59] 7 kube-system pods found
I1029 21:35:46.076191   78168 system_pods.go:61] "coredns-6f6b679f8f-2p4jk" [d5d3b39b-17ab-4cad-9395-3258a4f46ca7] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1029 21:35:46.076215   78168 system_pods.go:61] "etcd-minikube" [5fc17a18-1213-4474-b7ce-855bca1fd377] Running
I1029 21:35:46.076221   78168 system_pods.go:61] "kube-apiserver-minikube" [f7e6762d-8366-42a1-b49d-f09395334064] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1029 21:35:46.076224   78168 system_pods.go:61] "kube-controller-manager-minikube" [20862e7e-feff-44d4-8a98-6c9414c6d426] Running
I1029 21:35:46.076226   78168 system_pods.go:61] "kube-proxy-dzm8v" [7ffde221-f863-41aa-85fd-8b1ee47f946d] Running
I1029 21:35:46.076228   78168 system_pods.go:61] "kube-scheduler-minikube" [6b25cbcb-9aea-4632-b527-5dc18cbd5c21] Running
I1029 21:35:46.076230   78168 system_pods.go:61] "storage-provisioner" [2f2494be-cb96-4064-a961-d8ebf3bdb174] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1029 21:35:46.076576   78168 system_pods.go:74] duration metric: took 255.437024ms to wait for pod list to return data ...
I1029 21:35:46.076897   78168 kubeadm.go:582] duration metric: took 15.772598405s to wait for: map[apiserver:true system_pods:true]
I1029 21:35:46.078074   78168 node_conditions.go:102] verifying NodePressure condition ...
I1029 21:35:46.183750   78168 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1029 21:35:46.184014   78168 node_conditions.go:123] node cpu capacity is 12
I1029 21:35:46.185288   78168 node_conditions.go:105] duration metric: took 107.181141ms to run NodePressure ...
I1029 21:35:46.185655   78168 start.go:241] waiting for startup goroutines ...
I1029 21:35:46.619698   78168 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (11.398629578s)
I1029 21:35:46.628257   78168 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner
I1029 21:35:46.630779   78168 addons.go:510] duration metric: took 16.328172089s for enable addons: enabled=[default-storageclass storage-provisioner]
I1029 21:35:46.631209   78168 start.go:246] waiting for cluster config update ...
I1029 21:35:46.631637   78168 start.go:255] writing updated cluster config ...
I1029 21:35:46.639058   78168 ssh_runner.go:195] Run: rm -f paused
I1029 21:35:47.359965   78168 start.go:600] kubectl: 1.30.2, cluster: 1.31.0 (minor skew: 1)
I1029 21:35:47.362573   78168 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 30 00:33:18 minikube dockerd[55274]: time="2024-10-30T00:33:18.635400192Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Oct 30 00:33:18 minikube cri-dockerd[1647]: time="2024-10-30T00:33:18Z" level=error msg="error during connect: Get \"http://%2Fvar%2Frun%2Fdocker.sock/v1.43/images/json?all=1&shared-size=1\": read unix @->/var/run/docker.sock: read: connection reset by peerFailed to get image list from docker"
Oct 30 00:33:18 minikube systemd[1]: docker.service: Deactivated successfully.
Oct 30 00:33:18 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 30 00:33:18 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 30 00:33:18 minikube dockerd[55574]: time="2024-10-30T00:33:18.929103072Z" level=info msg="Starting up"
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.021327015Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.078465716Z" level=info msg="Loading containers: start."
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.600518939Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.701546778Z" level=info msg="Loading containers: done."
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.777325326Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.777372999Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.777381887Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.777392668Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.777430442Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Oct 30 00:33:19 minikube dockerd[55574]: time="2024-10-30T00:33:19.777543725Z" level=info msg="Daemon has completed initialization"
Oct 30 00:33:20 minikube dockerd[55574]: time="2024-10-30T00:33:20.254154674Z" level=info msg="API listen on /var/run/docker.sock"
Oct 30 00:33:20 minikube dockerd[55574]: time="2024-10-30T00:33:20.254252551Z" level=info msg="API listen on [::]:2376"
Oct 30 00:33:20 minikube systemd[1]: Started Docker Application Container Engine.
Oct 30 00:33:20 minikube cri-dockerd[1647]: time="2024-10-30T00:33:20Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"airports-deployment-f8f88784c-p8g9q_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0648721e4fb7595ccd1d682ef3b35e47e203148eb50d87c6b67da044d461a7f5\""
Oct 30 00:33:20 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Oct 30 00:33:20 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Oct 30 00:33:20 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Oct 30 00:33:21 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Oct 30 00:33:22 minikube cri-dockerd[55860]: time="2024-10-30T00:33:22Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Oct 30 00:33:22 minikube cri-dockerd[55860]: time="2024-10-30T00:33:22Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Oct 30 00:33:22 minikube cri-dockerd[55860]: time="2024-10-30T00:33:22Z" level=info msg="Start docker client with request timeout 0s"
Oct 30 00:33:22 minikube cri-dockerd[55860]: time="2024-10-30T00:33:22Z" level=info msg="Hairpin mode is set to hairpin-veth"
Oct 30 00:33:23 minikube cri-dockerd[55860]: time="2024-10-30T00:33:23Z" level=info msg="Loaded network plugin cni"
Oct 30 00:33:24 minikube cri-dockerd[55860]: time="2024-10-30T00:33:23Z" level=info msg="Docker cri networking managed by network plugin cni"
Oct 30 00:33:24 minikube cri-dockerd[55860]: time="2024-10-30T00:33:24Z" level=info msg="Setting cgroupDriver cgroupfs"
Oct 30 00:33:24 minikube cri-dockerd[55860]: time="2024-10-30T00:33:24Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Oct 30 00:33:24 minikube cri-dockerd[55860]: time="2024-10-30T00:33:24Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Oct 30 00:33:26 minikube cri-dockerd[55860]: time="2024-10-30T00:33:24Z" level=info msg="Start cri-dockerd grpc backend"
Oct 30 00:33:25 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Oct 30 00:33:29 minikube cri-dockerd[55860]: time="2024-10-30T00:33:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-2p4jk_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a47acd665dbb42276f252b192091ce18160dfa0fd217a88b7d65b2fdd2fea086\""
Oct 30 00:33:32 minikube cri-dockerd[55860]: time="2024-10-30T00:33:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"airports-deployment-f8f88784c-nc944_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d86c82d1d030723614c49b8a3f1a9d913d370682193b456de33c3ab5cb029940\""
Oct 30 00:34:30 minikube cri-dockerd[55860]: time="2024-10-30T00:34:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/96b044a5469b85bc4030b34fc1235e45f46527c89f6d569a4fa322b975a851a1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 30 00:34:30 minikube cri-dockerd[55860]: time="2024-10-30T00:34:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/355fa365ec90f74558dd3a01de23e408d985af87d3bdd0406d42cad38f3392ad/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 30 00:34:30 minikube cri-dockerd[55860]: time="2024-10-30T00:34:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fad977a2eac3a04daece6cf488dabf4951fcfc9707358657d60b007bfff8ec7d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 30 00:34:30 minikube cri-dockerd[55860]: time="2024-10-30T00:34:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a784033823cfd5191c560a9b0e8bb8e11898a3c22a6cf57edfebc112474ee61e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 30 00:34:30 minikube cri-dockerd[55860]: time="2024-10-30T00:34:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3bf219184f4110e1ec3bf62d43185a593e6c691f7b695b15e594c2edb3594009/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 30 00:34:30 minikube cri-dockerd[55860]: time="2024-10-30T00:34:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a72c00e4e73abf41014841992a20bba04e87f972a09aa3543cc697ee94eebdf4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 30 00:34:37 minikube cri-dockerd[55860]: time="2024-10-30T00:34:37Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/64abad8cba6697b1ae297bad98798f7ea6620ff896db1f5521a4301960952ea6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 30 00:34:38 minikube cri-dockerd[55860]: time="2024-10-30T00:34:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/623cfb31962f8e7779bc1e4aa1b7157f2d93b20fbb4df2ec19b2cfee43e929e2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 30 00:34:38 minikube cri-dockerd[55860]: time="2024-10-30T00:34:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6f6f10a1779ca4f5cb34128c00e72f78be5ac5d259148f7eb177af0f20300440/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 30 00:34:43 minikube dockerd[55574]: time="2024-10-30T00:34:43.612852533Z" level=info msg="ignoring event" container=2aa2cba5143a38fc0168523bd6ed24f0e2f50dcdb7b375aba6d7b43d4a97e4b7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 30 00:34:52 minikube dockerd[55574]: time="2024-10-30T00:34:52.640761484Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 30 00:34:52 minikube dockerd[55574]: time="2024-10-30T00:34:52.695020319Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 30 00:34:53 minikube cri-dockerd[55860]: time="2024-10-30T00:34:53Z" level=error msg="Failed to retrieve checkpoint for sandbox d86c82d1d030723614c49b8a3f1a9d913d370682193b456de33c3ab5cb029940: checkpoint is not found"
Oct 30 00:35:01 minikube dockerd[55574]: time="2024-10-30T00:35:01.662448125Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 30 00:35:01 minikube dockerd[55574]: time="2024-10-30T00:35:01.664882894Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 30 00:35:30 minikube dockerd[55574]: time="2024-10-30T00:35:30.303983621Z" level=info msg="ignoring event" container=b891832b6ea33d1b515c9b82e8177719d53b1205ddb81e76c433d349aededf57 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 30 00:35:39 minikube cri-dockerd[55860]: time="2024-10-30T00:35:39Z" level=error msg="error getting RW layer size for container ID '2aa2cba5143a38fc0168523bd6ed24f0e2f50dcdb7b375aba6d7b43d4a97e4b7': Error response from daemon: No such container: 2aa2cba5143a38fc0168523bd6ed24f0e2f50dcdb7b375aba6d7b43d4a97e4b7"
Oct 30 00:35:39 minikube cri-dockerd[55860]: time="2024-10-30T00:35:39Z" level=error msg="Set backoffDuration to : 1m0s for container ID '2aa2cba5143a38fc0168523bd6ed24f0e2f50dcdb7b375aba6d7b43d4a97e4b7'"
Oct 30 00:35:42 minikube dockerd[55574]: time="2024-10-30T00:35:42.723395559Z" level=info msg="ignoring event" container=49cb8dd1cda4cae6d61beb84d45014f14b6cf841cda2737a6808fa6cbf5561b5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 30 00:40:12 minikube dockerd[55574]: time="2024-10-30T00:40:12.833295363Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://registry-1.docker.io/v2/library/asa_container/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%3Alibrary%2Fasa_container%3Apull&service=registry.docker.io\": net/http: TLS handshake timeout"
Oct 30 00:40:12 minikube dockerd[55574]: time="2024-10-30T00:40:12.840246422Z" level=error msg="Handler for POST /v1.43/images/create returned error: Head \"https://registry-1.docker.io/v2/library/asa_container/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%3Alibrary%2Fasa_container%3Apull&service=registry.docker.io\": net/http: TLS handshake timeout"
Oct 30 00:40:17 minikube dockerd[55574]: time="2024-10-30T00:40:17.775119557Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 30 00:40:17 minikube dockerd[55574]: time="2024-10-30T00:40:17.775300713Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
4eda9a39d6e9b       6e38f40d628db       8 minutes ago       Running             storage-provisioner       4                   fad977a2eac3a       storage-provisioner
5bd66c505f3d0       045733566833c       8 minutes ago       Running             kube-controller-manager   2                   355fa365ec90f       kube-controller-manager-minikube
b891832b6ea33       6e38f40d628db       9 minutes ago       Exited              storage-provisioner       3                   fad977a2eac3a       storage-provisioner
7c1a983f730c1       cbb01a7bd410d       9 minutes ago       Running             coredns                   1                   623cfb31962f8       coredns-6f6b679f8f-2p4jk
89851d6c3ccf7       ad83b2ca7b09e       9 minutes ago       Running             kube-proxy                1                   a72c00e4e73ab       kube-proxy-dzm8v
bb09446b146cf       604f5db92eaa8       9 minutes ago       Running             kube-apiserver            1                   96b044a5469b8       kube-apiserver-minikube
745643406fb87       2e96e5913fc06       9 minutes ago       Running             etcd                      1                   3bf219184f411       etcd-minikube
49cb8dd1cda4c       045733566833c       9 minutes ago       Exited              kube-controller-manager   1                   355fa365ec90f       kube-controller-manager-minikube
73954bc04967b       1766f54c897f0       9 minutes ago       Running             kube-scheduler            1                   a784033823cfd       kube-scheduler-minikube
e4b3154cd99d2       cbb01a7bd410d       6 hours ago         Exited              coredns                   0                   a47acd665dbb4       coredns-6f6b679f8f-2p4jk
f3f5c8b9daafe       ad83b2ca7b09e       6 hours ago         Exited              kube-proxy                0                   263bbcf7f4136       kube-proxy-dzm8v
e94937944de62       604f5db92eaa8       6 hours ago         Exited              kube-apiserver            0                   752891db86e71       kube-apiserver-minikube
2e5b0d4581a33       2e96e5913fc06       6 hours ago         Exited              etcd                      0                   3a25f859a653b       etcd-minikube
aad3d67d99716       1766f54c897f0       6 hours ago         Exited              kube-scheduler            0                   b9fc9dd580927       kube-scheduler-minikube


==> coredns [7c1a983f730c] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:33682 - 4959 "HINFO IN 2587806493628708742.1529369535337284412. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.576167244s
[INFO] 127.0.0.1:40456 - 25767 "HINFO IN 2587806493628708742.1529369535337284412. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.101309619s
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1990127859]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (30-Oct-2024 00:35:17.066) (total time: 11850ms):
Trace[1990127859]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 11847ms (00:35:28.914)
Trace[1990127859]: [11.850510772s] [11.850510772s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[871635212]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (30-Oct-2024 00:35:17.066) (total time: 12002ms):
Trace[871635212]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 11911ms (00:35:28.977)
Trace[871635212]: [12.002142462s] [12.002142462s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/kubernetes: Trace[1387090970]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (30-Oct-2024 00:35:17.066) (total time: 12004ms):
Trace[1387090970]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout 12004ms (00:35:29.070)
Trace[1387090970]: [12.004834344s] [12.004834344s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": net/http: TLS handshake timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: Trace[338122923]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (30-Oct-2024 00:35:30.821) (total time: 10288ms):
Trace[338122923]: ---"Objects listed" error:<nil> 10286ms (00:35:41.108)
Trace[338122923]: [10.288202396s] [10.288202396s] END
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": dial tcp :8080: i/o timeout (Client.Timeout exceeded while awaiting headers)


==> coredns [e4b3154cd99d] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:56215 - 29048 "HINFO IN 2859144296621219297.6781526772375974110. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.189987585s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1562057707]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (29-Oct-2024 19:01:57.890) (total time: 21943ms):
Trace[1562057707]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21940ms (19:02:18.707)
Trace[1562057707]: [21.94321688s] [21.94321688s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[584649474]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (29-Oct-2024 19:01:57.890) (total time: 21955ms):
Trace[584649474]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21955ms (19:02:18.722)
Trace[584649474]: [21.95567145s] [21.95567145s] END
[INFO] plugin/kubernetes: Trace[929588234]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (29-Oct-2024 19:01:57.890) (total time: 21955ms):
Trace[929588234]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21954ms (19:02:18.721)
Trace[929588234]: [21.955799358s] [21.955799358s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.50123575s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.100210188s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.702251477s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.104240134s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.405595053s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_10_29T16_01_50_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 29 Oct 2024 19:01:47 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 30 Oct 2024 00:44:05 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 30 Oct 2024 00:41:28 +0000   Tue, 29 Oct 2024 19:01:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 30 Oct 2024 00:41:28 +0000   Tue, 29 Oct 2024 19:01:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 30 Oct 2024 00:41:28 +0000   Tue, 29 Oct 2024 19:01:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 30 Oct 2024 00:41:28 +0000   Tue, 29 Oct 2024 19:01:47 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3745728Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3745728Ki
  pods:               110
System Info:
  Machine ID:                 901b3a2dd16b483098096a9844698e1e
  System UUID:                901b3a2dd16b483098096a9844698e1e
  Boot ID:                    2a93bfcb-aad4-410a-aa8d-f27ea30eb9f7
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     airports-deployment-66bf4c5587-cbhw8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39s
  default                     airports-deployment-f8f88784c-nc944     0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m
  default                     airports-deployment-f8f88784c-p8g9q     0 (0%)        0 (0%)      0 (0%)           0 (0%)         23m
  default                     flights-deployment-7c55dd7765-24hg8     0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m
  default                     flights-deployment-7c55dd7765-cpg7h     0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m
  default                     users-deployment-65d99cf9bc-296gl       0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m
  default                     users-deployment-65d99cf9bc-jpkjm       0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m
  kube-system                 coredns-6f6b679f8f-2p4jk                100m (0%)     0 (0%)      70Mi (1%)        170Mi (4%)     5h42m
  kube-system                 etcd-minikube                           100m (0%)     0 (0%)      100Mi (2%)       0 (0%)         5h42m
  kube-system                 kube-apiserver-minikube                 250m (2%)     0 (0%)      0 (0%)           0 (0%)         5h42m
  kube-system                 kube-controller-manager-minikube        200m (1%)     0 (0%)      0 (0%)           0 (0%)         5h42m
  kube-system                 kube-proxy-dzm8v                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h42m
  kube-system                 kube-scheduler-minikube                 100m (0%)     0 (0%)      0 (0%)           0 (0%)         5h42m
  kube-system                 storage-provisioner                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h42m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason          Age    From             Message
  ----    ------          ----   ----             -------
  Normal  Starting        8m28s  kube-proxy       
  Normal  RegisteredNode  8m14s  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000971] CPU: 0 PID: 110145 Comm: kworker/0:2 Not tainted 5.15.153.1-microsoft-standard-WSL2 #1
[  +0.001086] Workqueue: hv_pri_chan vmbus_add_channel_work
[  +0.002577] Call Trace:
[  +0.002783]  <TASK>
[  +0.000895]  dump_stack_lvl+0x34/0x48
[  +0.000804]  warn_alloc+0x131/0x150
[  +0.001348]  ? srso_alias_return_thunk+0x5/0x7f
[  +0.000667]  ? __alloc_pages_direct_compact+0x146/0x1b0
[  +0.000006]  __alloc_pages_slowpath.constprop.0+0xc25/0xca0
[  +0.000005]  ? srso_alias_return_thunk+0x5/0x7f
[  +0.000734]  __alloc_pages+0x2c0/0x2e0
[  +0.000008]  ? vsock_diag_dump+0x290/0x290
[  +0.000252]  vmbus_alloc_ring+0x4b/0xa0
[  +0.000070]  vmbus_open+0x21/0x70
[  +0.000029]  ? srso_alias_return_thunk+0x5/0x7f
[  +0.000003]  hvs_probe+0x405/0x4c0
[  +0.000047]  vmbus_probe+0x5f/0x80
[  +0.000004]  really_probe.part.0+0xa5/0x290
[  +0.000882]  driver_probe_device+0x1e/0x90
[  +0.000005]  __device_attach_driver+0x7f/0xc0
[  +0.000013]  ? driver_allows_async_probing+0x50/0x50
[  +0.000002]  ? driver_allows_async_probing+0x50/0x50
[  +0.000002]  bus_for_each_drv+0x7e/0xd0
[  +0.000109]  __device_attach+0x105/0x180
[  +0.000005]  bus_probe_device+0x8e/0xb0
[  +0.000003]  device_add+0x3a8/0x8e0
[  +0.000015]  ? srso_alias_return_thunk+0x5/0x7f
[  +0.000002]  ? dev_set_name+0x53/0x70
[  +0.000013]  vmbus_device_register+0x59/0xc0
[  +0.000003]  vmbus_add_channel_work+0x132/0x1a0
[  +0.000002]  process_one_work+0x1eb/0x390
[  +0.000627]  worker_thread+0x53/0x3c0
[  +0.000003]  ? process_one_work+0x390/0x390
[  +0.000002]  kthread+0x127/0x150
[  +0.000049]  ? set_kthread_struct+0x50/0x50
[  +0.000004]  ret_from_fork+0x22/0x30
[  +0.000868]  </TASK>
[  +0.000505] Mem-Info:
[  +0.000016] active_anon:132147 inactive_anon:287599 isolated_anon:0
               active_file:194802 inactive_file:136526 isolated_file:63
               unevictable:0 dirty:26 writeback:0
               slab_reclaimable:54720 slab_unreclaimable:27299
               mapped:110390 shmem:10429 pagetables:7286 bounce:0
               kernel_misc_reclaimable:0
               free:25177 free_pcp:184 free_cma:0
[  +0.000006] Node 0 active_anon:528588kB inactive_anon:1150396kB active_file:779208kB inactive_file:546104kB unevictable:0kB isolated(anon):0kB isolated(file):252kB mapped:441560kB dirty:104kB writeback:0kB shmem:41716kB shmem_thp: 0kB shmem_pmdmapped: 0kB anon_thp: 704512kB writeback_tmp:0kB kernel_stack:19232kB pagetables:29144kB all_unreclaimable? no
[  +0.000025] DMA free:14336kB min:172kB low:212kB high:252kB reserved_highatomic:0KB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB writepending:0kB present:14972kB managed:14336kB mlocked:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB
[  +0.000004] lowmem_reserve[]: 0 3637 3637 3637 3637
[  +0.000004] DMA32 free:86372kB min:44880kB low:56100kB high:67320kB reserved_highatomic:2048KB active_anon:528332kB inactive_anon:1150208kB active_file:779492kB inactive_file:546108kB unevictable:0kB writepending:176kB present:3840000kB managed:3731392kB mlocked:0kB bounce:0kB free_pcp:736kB local_pcp:0kB free_cma:0kB
[  +0.000004] lowmem_reserve[]: 0 0 0 0 0
[  +0.000003] DMA: 0*4kB 0*8kB 0*16kB 0*32kB 0*64kB 0*128kB 0*256kB 0*512kB 0*1024kB 1*2048kB (M) 3*4096kB (M) = 14336kB
[  +0.000010] DMA32: 6255*4kB (UMEH) 1600*8kB (UMEH) 621*16kB (UMEH) 819*32kB (UMEH) 188*64kB (UME) 14*128kB (UME) 3*256kB (M) 0*512kB 0*1024kB 0*2048kB 0*4096kB = 88556kB
[  +0.000018] 352674 total pagecache pages
[  +0.000001] 10846 pages in swap cache
[  +0.000002] Swap cache stats: add 228307, delete 217476, find 72124/96091
[  +0.000002] Free swap  = 536244kB
[  +0.000001] Total swap = 1048576kB
[  +0.000000] 963743 pages RAM
[  +0.000001] 0 pages HighMem/MovableOnly
[  +0.000001] 27311 pages reserved


==> etcd [2e5b0d4581a3] <==
{"level":"warn","ts":"2024-10-30T00:32:40.394591Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"579.081298ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:32:40.396600Z","caller":"traceutil/trace.go:171","msg":"trace[61428944] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12341; }","duration":"587.162809ms","start":"2024-10-30T00:32:39.808875Z","end":"2024-10-30T00:32:40.396038Z","steps":["trace[61428944] 'agreement among raft nodes before linearized reading'  (duration: 578.605674ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:32:40.487521Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"204.60808ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:133"}
{"level":"info","ts":"2024-10-30T00:32:40.490705Z","caller":"traceutil/trace.go:171","msg":"trace[595015559] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:12342; }","duration":"298.19117ms","start":"2024-10-30T00:32:40.191881Z","end":"2024-10-30T00:32:40.490072Z","steps":["trace[595015559] 'agreement among raft nodes before linearized reading'  (duration: 294.526738ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:32:40.491110Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:40.188605Z","time spent":"302.405833ms","remote":"127.0.0.1:37656","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":157,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"info","ts":"2024-10-30T00:32:40.493382Z","caller":"traceutil/trace.go:171","msg":"trace[36128925] transaction","detail":"{read_only:false; response_revision:12342; number_of_response:1; }","duration":"205.829408ms","start":"2024-10-30T00:32:40.287426Z","end":"2024-10-30T00:32:40.493255Z","steps":["trace[36128925] 'process raft request'  (duration: 196.639491ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:32:42.286643Z","caller":"traceutil/trace.go:171","msg":"trace[1109896682] linearizableReadLoop","detail":"{readStateIndex:15377; appliedIndex:15377; }","duration":"195.153073ms","start":"2024-10-30T00:32:42.087382Z","end":"2024-10-30T00:32:42.282535Z","steps":["trace[1109896682] 'read index received'  (duration: 195.132052ms)","trace[1109896682] 'applied index is now lower than readState.Index'  (duration: 17.554¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:32:42.293592Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"189.997866ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:32:42.293849Z","caller":"traceutil/trace.go:171","msg":"trace[1610144142] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12342; }","duration":"294.154333ms","start":"2024-10-30T00:32:41.999623Z","end":"2024-10-30T00:32:42.293777Z","steps":["trace[1610144142] 'agreement among raft nodes before linearized reading'  (duration: 87.780216ms)","trace[1610144142] 'range keys from in-memory index tree'  (duration: 102.040904ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:32:42.195083Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.973583ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:32:42.295493Z","caller":"traceutil/trace.go:171","msg":"trace[2070605322] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12342; }","duration":"205.299477ms","start":"2024-10-30T00:32:42.090141Z","end":"2024-10-30T00:32:42.295440Z","steps":["trace[2070605322] 'range keys from in-memory index tree'  (duration: 100.929497ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:32:42.595685Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"512.257035ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-10-30T00:32:42.596511Z","caller":"traceutil/trace.go:171","msg":"trace[509671795] range","detail":"{range_begin:/registry/secrets/; range_end:/registry/secrets0; response_count:0; response_revision:12342; }","duration":"513.446299ms","start":"2024-10-30T00:32:42.082805Z","end":"2024-10-30T00:32:42.596251Z","steps":["trace[509671795] 'agreement among raft nodes before linearized reading'  (duration: 204.478903ms)","trace[509671795] 'count revisions from in-memory index tree'  (duration: 306.469844ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:32:42.596940Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:42.082700Z","time spent":"514.178565ms","remote":"127.0.0.1:37732","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":1,"response size":31,"request content":"key:\"/registry/secrets/\" range_end:\"/registry/secrets0\" count_only:true "}
{"level":"warn","ts":"2024-10-30T00:32:42.693714Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"207.491342ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032891017159365 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:12336 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2024-10-30T00:32:42.694656Z","caller":"traceutil/trace.go:171","msg":"trace[1919565703] transaction","detail":"{read_only:false; response_revision:12343; number_of_response:1; }","duration":"598.410066ms","start":"2024-10-30T00:32:42.096183Z","end":"2024-10-30T00:32:42.694593Z","steps":["trace[1919565703] 'process raft request'  (duration: 294.981648ms)","trace[1919565703] 'compare'  (duration: 205.160689ms)"],"step_count":2}
{"level":"info","ts":"2024-10-30T00:32:42.694876Z","caller":"traceutil/trace.go:171","msg":"trace[969272670] linearizableReadLoop","detail":"{readStateIndex:15378; appliedIndex:15377; }","duration":"404.396855ms","start":"2024-10-30T00:32:42.290438Z","end":"2024-10-30T00:32:42.694835Z","steps":["trace[969272670] 'read index received'  (duration: 98.833096ms)","trace[969272670] 'applied index is now lower than readState.Index'  (duration: 305.562957ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:32:42.695026Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:42.096048Z","time spent":"598.728902ms","remote":"127.0.0.1:37860","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:12336 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-10-30T00:32:42.787643Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"602.803571ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:32:42.787557Z","caller":"traceutil/trace.go:171","msg":"trace[1972373367] transaction","detail":"{read_only:false; response_revision:12344; number_of_response:1; }","duration":"488.234668ms","start":"2024-10-30T00:32:42.299226Z","end":"2024-10-30T00:32:42.787461Z","steps":["trace[1972373367] 'process raft request'  (duration: 394.787792ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:32:42.787694Z","caller":"traceutil/trace.go:171","msg":"trace[1600355836] range","detail":"{range_begin:/registry/runtimeclasses/; range_end:/registry/runtimeclasses0; response_count:0; response_revision:12344; }","duration":"602.995146ms","start":"2024-10-30T00:32:42.184677Z","end":"2024-10-30T00:32:42.787673Z","steps":["trace[1600355836] 'agreement among raft nodes before linearized reading'  (duration: 510.410477ms)","trace[1600355836] 'count revisions from in-memory index tree'  (duration: 91.890011ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:32:42.787748Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:42.184326Z","time spent":"603.413988ms","remote":"127.0.0.1:37890","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":0,"response size":29,"request content":"key:\"/registry/runtimeclasses/\" range_end:\"/registry/runtimeclasses0\" count_only:true "}
{"level":"warn","ts":"2024-10-30T00:32:42.787800Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:42.299115Z","time spent":"488.579049ms","remote":"127.0.0.1:37656","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:12338 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128032891017159362 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2024-10-30T00:32:42.788228Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"700.444763ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:32:42.788349Z","caller":"traceutil/trace.go:171","msg":"trace[946380548] range","detail":"{range_begin:/registry/statefulsets/; range_end:/registry/statefulsets0; response_count:0; response_revision:12344; }","duration":"700.516905ms","start":"2024-10-30T00:32:42.087733Z","end":"2024-10-30T00:32:42.788250Z","steps":["trace[946380548] 'agreement among raft nodes before linearized reading'  (duration: 607.878225ms)","trace[946380548] 'count revisions from in-memory index tree'  (duration: 92.228811ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:32:42.788416Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:42.087634Z","time spent":"700.771083ms","remote":"127.0.0.1:38064","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":0,"response size":29,"request content":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true "}
{"level":"info","ts":"2024-10-30T00:32:42.788886Z","caller":"traceutil/trace.go:171","msg":"trace[476933144] transaction","detail":"{read_only:false; response_revision:12345; number_of_response:1; }","duration":"383.092512ms","start":"2024-10-30T00:32:42.405785Z","end":"2024-10-30T00:32:42.788878Z","steps":["trace[476933144] 'process raft request'  (duration: 381.448616ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:32:42.789078Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:42.405597Z","time spent":"383.309147ms","remote":"127.0.0.1:37782","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:12341 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-10-30T00:32:42.893310Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"597.444794ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:32:42.893393Z","caller":"traceutil/trace.go:171","msg":"trace[752681457] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12345; }","duration":"597.54464ms","start":"2024-10-30T00:32:42.295828Z","end":"2024-10-30T00:32:42.893373Z","steps":["trace[752681457] 'agreement among raft nodes before linearized reading'  (duration: 493.013592ms)","trace[752681457] 'range keys from in-memory index tree'  (duration: 104.39402ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:32:42.896654Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"400.492814ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-10-30T00:32:42.896773Z","caller":"traceutil/trace.go:171","msg":"trace[653717418] range","detail":"{range_begin:/registry/storageclasses/; range_end:/registry/storageclasses0; response_count:0; response_revision:12345; }","duration":"400.566359ms","start":"2024-10-30T00:32:42.496117Z","end":"2024-10-30T00:32:42.896683Z","steps":["trace[653717418] 'agreement among raft nodes before linearized reading'  (duration: 292.965425ms)","trace[653717418] 'count revisions from in-memory index tree'  (duration: 107.315774ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:32:42.896845Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:42.495287Z","time spent":"401.547879ms","remote":"127.0.0.1:37952","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":1,"response size":31,"request content":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" count_only:true "}
{"level":"warn","ts":"2024-10-30T00:32:43.783602Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"199.98229ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032891017159378 > lease_revoke:<id:70cc92d9a7c4e29c>","response":"size:29"}
{"level":"warn","ts":"2024-10-30T00:32:47.688729Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"194.149389ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:32:47.690600Z","caller":"traceutil/trace.go:171","msg":"trace[1765229688] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12346; }","duration":"701.436948ms","start":"2024-10-30T00:32:46.989009Z","end":"2024-10-30T00:32:47.690446Z","steps":["trace[1765229688] 'get authentication metadata'  (duration: 193.981991ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:32:47.690753Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:46.988939Z","time spent":"701.788062ms","remote":"127.0.0.1:37626","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-10-30T00:32:49.400407Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"208.373853ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032891017159398 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc92d9a7c4e2e5>","response":"size:41"}
{"level":"warn","ts":"2024-10-30T00:32:49.401049Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:32:49.094009Z","time spent":"306.99704ms","remote":"127.0.0.1:37656","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-10-30T00:32:59.298862Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"287.674246ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:32:59.302048Z","caller":"traceutil/trace.go:171","msg":"trace[1551408383] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12354; }","duration":"291.401774ms","start":"2024-10-30T00:32:59.008973Z","end":"2024-10-30T00:32:59.300374Z","steps":["trace[1551408383] 'agreement among raft nodes before linearized reading'  (duration: 89.498349ms)","trace[1551408383] 'range keys from in-memory index tree'  (duration: 198.110248ms)"],"step_count":2}
{"level":"info","ts":"2024-10-30T00:32:59.302896Z","caller":"traceutil/trace.go:171","msg":"trace[1213576650] transaction","detail":"{read_only:false; response_revision:12355; number_of_response:1; }","duration":"196.915433ms","start":"2024-10-30T00:32:59.105905Z","end":"2024-10-30T00:32:59.302821Z","steps":["trace[1213576650] 'process raft request'  (duration: 189.726184ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:32:59.513997Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"116.339124ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:32:59.514134Z","caller":"traceutil/trace.go:171","msg":"trace[853376503] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12355; }","duration":"116.846871ms","start":"2024-10-30T00:32:59.397261Z","end":"2024-10-30T00:32:59.514108Z","steps":["trace[853376503] 'range keys from in-memory index tree'  (duration: 116.25037ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:33:00.500753Z","caller":"traceutil/trace.go:171","msg":"trace[1891209038] transaction","detail":"{read_only:false; response_revision:12356; number_of_response:1; }","duration":"102.575838ms","start":"2024-10-30T00:33:00.398148Z","end":"2024-10-30T00:33:00.500724Z","steps":["trace[1891209038] 'process raft request'  (duration: 98.345964ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:33:00.607507Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"warn","ts":"2024-10-30T00:33:00.999103Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"191.569782ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:33:01.000576Z","caller":"traceutil/trace.go:171","msg":"trace[1161302232] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12356; }","duration":"195.01262ms","start":"2024-10-30T00:33:00.804843Z","end":"2024-10-30T00:33:00.999856Z","steps":["trace[1161302232] 'range keys from in-memory index tree'  (duration: 191.454225ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:33:00.799392Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-10-30T00:33:01.702291Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-10-30T00:33:03.105667Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-10-30T00:33:05.098082Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"298.596257ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032891017159465 > lease_revoke:<id:70cc92d9a7c4e2e5>","response":"size:29"}
{"level":"warn","ts":"2024-10-30T00:33:05.098050Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.313131ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:33:05.098683Z","caller":"traceutil/trace.go:171","msg":"trace[367008758] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12356; }","duration":"202.138324ms","start":"2024-10-30T00:33:04.896491Z","end":"2024-10-30T00:33:05.098630Z","steps":["trace[367008758] 'range keys from in-memory index tree'  (duration: 200.295817ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:33:05.307202Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-10-30T00:33:05.308638Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-10-30T00:33:05.310179Z","caller":"etcdserver/server.go:1521","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-10-30T00:33:06.295490Z","caller":"embed/etcd.go:581","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-10-30T00:33:06.407491Z","caller":"embed/etcd.go:586","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-10-30T00:33:06.407756Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [745643406fb8] <==
{"level":"info","ts":"2024-10-30T00:34:59.183598Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-30T00:34:59.183590Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-30T00:34:59.183730Z","caller":"traceutil/trace.go:171","msg":"trace[363369534] linearizableReadLoop","detail":"{readStateIndex:15398; appliedIndex:14161; }","duration":"257.12677ms","start":"2024-10-30T00:34:58.926417Z","end":"2024-10-30T00:34:59.183544Z","steps":["trace[363369534] 'read index received'  (duration: 838.793¬µs)","trace[363369534] 'applied index is now lower than readState.Index'  (duration: 256.286045ms)"],"step_count":2}
{"level":"info","ts":"2024-10-30T00:34:59.185161Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-10-30T00:34:59.185340Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"warn","ts":"2024-10-30T00:34:59.187107Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"257.854633ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:34:59.187173Z","caller":"traceutil/trace.go:171","msg":"trace[1167414356] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12356; }","duration":"260.837295ms","start":"2024-10-30T00:34:58.926303Z","end":"2024-10-30T00:34:59.187140Z","steps":["trace[1167414356] 'agreement among raft nodes before linearized reading'  (duration: 257.773791ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:34:59.266925Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-30T00:34:59.268953Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-10-30T00:34:59.290137Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-30T00:34:59.292329Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"warn","ts":"2024-10-30T00:35:41.210856Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"300.70684ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032896134072229 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:12350 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2024-10-30T00:35:41.218812Z","caller":"traceutil/trace.go:171","msg":"trace[1120344201] transaction","detail":"{read_only:false; response_revision:12358; number_of_response:1; }","duration":"607.728467ms","start":"2024-10-30T00:35:40.610704Z","end":"2024-10-30T00:35:41.218432Z","steps":["trace[1120344201] 'process raft request'  (duration: 95.526514ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:35:41.220824Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:35:40.609276Z","time spent":"611.260067ms","remote":"127.0.0.1:34120","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:12350 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2024-10-30T00:35:41.228610Z","caller":"traceutil/trace.go:171","msg":"trace[198643162] linearizableReadLoop","detail":"{readStateIndex:15403; appliedIndex:15400; }","duration":"600.137173ms","start":"2024-10-30T00:35:40.628348Z","end":"2024-10-30T00:35:41.228485Z","steps":["trace[198643162] 'read index received'  (duration: 75.885166ms)","trace[198643162] 'applied index is now lower than readState.Index'  (duration: 524.250985ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:35:41.229087Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:35:40.616827Z","time spent":"612.108352ms","remote":"127.0.0.1:33944","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-10-30T00:35:41.307138Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"301.342739ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/system-node-critical\" ","response":"range_response_count:1 size:442"}
{"level":"info","ts":"2024-10-30T00:35:41.307565Z","caller":"traceutil/trace.go:171","msg":"trace[1738507869] range","detail":"{range_begin:/registry/priorityclasses/system-node-critical; range_end:; response_count:1; response_revision:12359; }","duration":"301.491493ms","start":"2024-10-30T00:35:41.005714Z","end":"2024-10-30T00:35:41.307206Z","steps":["trace[1738507869] 'agreement among raft nodes before linearized reading'  (duration: 297.341947ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:35:41.308066Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:35:41.005548Z","time spent":"302.474758ms","remote":"127.0.0.1:34212","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":1,"response size":466,"request content":"key:\"/registry/priorityclasses/system-node-critical\" "}
{"level":"info","ts":"2024-10-30T00:35:41.308984Z","caller":"traceutil/trace.go:171","msg":"trace[1315094560] transaction","detail":"{read_only:false; response_revision:12359; number_of_response:1; }","duration":"683.345459ms","start":"2024-10-30T00:35:40.625625Z","end":"2024-10-30T00:35:41.308970Z","steps":["trace[1315094560] 'process raft request'  (duration: 601.291327ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:35:41.309494Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:35:40.625600Z","time spent":"683.84587ms","remote":"127.0.0.1:34120","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:12351 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2024-10-30T00:35:41.409874Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"781.433401ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" ","response":"range_response_count:1 size:4389"}
{"level":"info","ts":"2024-10-30T00:35:41.410322Z","caller":"traceutil/trace.go:171","msg":"trace[1059690142] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:12359; }","duration":"781.889437ms","start":"2024-10-30T00:35:40.628341Z","end":"2024-10-30T00:35:41.410230Z","steps":["trace[1059690142] 'agreement among raft nodes before linearized reading'  (duration: 675.567963ms)","trace[1059690142] 'range keys from in-memory index tree'  (duration: 105.630582ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:35:41.411675Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:35:40.628240Z","time spent":"783.40887ms","remote":"127.0.0.1:34044","response type":"/etcdserverpb.KV/Range","request count":0,"request size":28,"response count":1,"response size":4413,"request content":"key:\"/registry/minions/minikube\" "}
{"level":"warn","ts":"2024-10-30T00:35:41.419090Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"412.261011ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:35:41.419159Z","caller":"traceutil/trace.go:171","msg":"trace[1041878279] range","detail":"{range_begin:/registry/clusterroles; range_end:; response_count:0; response_revision:12359; }","duration":"412.346062ms","start":"2024-10-30T00:35:41.006795Z","end":"2024-10-30T00:35:41.419141Z","steps":["trace[1041878279] 'agreement among raft nodes before linearized reading'  (duration: 301.35672ms)","trace[1041878279] 'range keys from in-memory index tree'  (duration: 110.84512ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:35:41.419554Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:35:41.006745Z","time spent":"412.782769ms","remote":"127.0.0.1:34188","response type":"/etcdserverpb.KV/Range","request count":0,"request size":26,"response count":0,"response size":29,"request content":"key:\"/registry/clusterroles\" limit:1 "}
{"level":"info","ts":"2024-10-30T00:35:41.816182Z","caller":"traceutil/trace.go:171","msg":"trace[2082931125] transaction","detail":"{read_only:false; response_revision:12362; number_of_response:1; }","duration":"105.801106ms","start":"2024-10-30T00:35:41.710329Z","end":"2024-10-30T00:35:41.816130Z","steps":["trace[2082931125] 'process raft request'  (duration: 102.754743ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:35:41.816408Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.553358ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:aggregate-to-admin\" ","response":"range_response_count:1 size:840"}
{"level":"info","ts":"2024-10-30T00:35:41.816468Z","caller":"traceutil/trace.go:171","msg":"trace[965774253] range","detail":"{range_begin:/registry/clusterroles/system:aggregate-to-admin; range_end:; response_count:1; response_revision:12362; }","duration":"104.650841ms","start":"2024-10-30T00:35:41.711808Z","end":"2024-10-30T00:35:41.816459Z","steps":["trace[965774253] 'agreement among raft nodes before linearized reading'  (duration: 104.401124ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:35:41.816065Z","caller":"traceutil/trace.go:171","msg":"trace[1651838733] linearizableReadLoop","detail":"{readStateIndex:15406; appliedIndex:15405; }","duration":"104.178034ms","start":"2024-10-30T00:35:41.711819Z","end":"2024-10-30T00:35:41.815997Z","steps":["trace[1651838733] 'read index received'  (duration: 17.175543ms)","trace[1651838733] 'applied index is now lower than readState.Index'  (duration: 86.998788ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:35:43.603416Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"299.04139ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:service-controller\" ","response":"range_response_count:1 size:724"}
{"level":"info","ts":"2024-10-30T00:35:43.603643Z","caller":"traceutil/trace.go:171","msg":"trace[346819729] range","detail":"{range_begin:/registry/clusterroles/system:controller:service-controller; range_end:; response_count:1; response_revision:12396; }","duration":"299.355197ms","start":"2024-10-30T00:35:43.304239Z","end":"2024-10-30T00:35:43.603595Z","steps":["trace[346819729] 'range keys from in-memory index tree'  (duration: 298.931231ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:35:43.604537Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"198.813658ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:35:43.604603Z","caller":"traceutil/trace.go:171","msg":"trace[2080868460] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12396; }","duration":"199.091657ms","start":"2024-10-30T00:35:43.405478Z","end":"2024-10-30T00:35:43.604570Z","steps":["trace[2080868460] 'range keys from in-memory index tree'  (duration: 198.785342ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:35:43.605826Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"292.070737ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032896134072372 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/kube-controller-manager-minikube.180313bdb8321bad\" mod_revision:12363 > success:<request_put:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.180313bdb8321bad\" value_size:753 lease:8128032896134072230 >> failure:<request_range:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.180313bdb8321bad\" > >>","response":"size:16"}
{"level":"info","ts":"2024-10-30T00:35:43.606075Z","caller":"traceutil/trace.go:171","msg":"trace[1187613559] transaction","detail":"{read_only:false; response_revision:12397; number_of_response:1; }","duration":"301.791389ms","start":"2024-10-30T00:35:43.304253Z","end":"2024-10-30T00:35:43.606044Z","steps":["trace[1187613559] 'compare'  (duration: 290.76495ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:35:43.606482Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:35:43.304237Z","time spent":"301.971251ms","remote":"127.0.0.1:33944","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":849,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/kube-controller-manager-minikube.180313bdb8321bad\" mod_revision:12363 > success:<request_put:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.180313bdb8321bad\" value_size:753 lease:8128032896134072230 >> failure:<request_range:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.180313bdb8321bad\" > >"}
{"level":"info","ts":"2024-10-30T00:35:43.607662Z","caller":"traceutil/trace.go:171","msg":"trace[691321123] transaction","detail":"{read_only:false; response_revision:12398; number_of_response:1; }","duration":"303.109001ms","start":"2024-10-30T00:35:43.304541Z","end":"2024-10-30T00:35:43.607650Z","steps":["trace[691321123] 'process raft request'  (duration: 301.422862ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:35:43.608408Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:35:43.304530Z","time spent":"303.831031ms","remote":"127.0.0.1:34048","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4924,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/kube-system/kube-proxy-dzm8v\" mod_revision:361 > success:<request_put:<key:\"/registry/pods/kube-system/kube-proxy-dzm8v\" value_size:4873 >> failure:<request_range:<key:\"/registry/pods/kube-system/kube-proxy-dzm8v\" > >"}
{"level":"info","ts":"2024-10-30T00:35:45.908075Z","caller":"traceutil/trace.go:171","msg":"trace[1305021995] linearizableReadLoop","detail":"{readStateIndex:15484; appliedIndex:15483; }","duration":"273.24682ms","start":"2024-10-30T00:35:45.634718Z","end":"2024-10-30T00:35:45.907965Z","steps":["trace[1305021995] 'read index received'  (duration: 272.122838ms)","trace[1305021995] 'applied index is now lower than readState.Index'  (duration: 1.122484ms)"],"step_count":2}
{"level":"info","ts":"2024-10-30T00:35:45.909593Z","caller":"traceutil/trace.go:171","msg":"trace[1216000585] transaction","detail":"{read_only:false; response_revision:12438; number_of_response:1; }","duration":"278.273457ms","start":"2024-10-30T00:35:45.631294Z","end":"2024-10-30T00:35:45.909567Z","steps":["trace[1216000585] 'process raft request'  (duration: 275.801128ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:35:45.911283Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"276.078267ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/storage-provisioner\" ","response":"range_response_count:1 size:4531"}
{"level":"info","ts":"2024-10-30T00:35:45.911619Z","caller":"traceutil/trace.go:171","msg":"trace[2121026994] range","detail":"{range_begin:/registry/pods/kube-system/storage-provisioner; range_end:; response_count:1; response_revision:12438; }","duration":"276.718079ms","start":"2024-10-30T00:35:45.634711Z","end":"2024-10-30T00:35:45.911429Z","steps":["trace[2121026994] 'agreement among raft nodes before linearized reading'  (duration: 275.611671ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:36:21.383599Z","caller":"traceutil/trace.go:171","msg":"trace[722257160] transaction","detail":"{read_only:false; response_revision:12511; number_of_response:1; }","duration":"900.7384ms","start":"2024-10-30T00:36:20.480459Z","end":"2024-10-30T00:36:21.381197Z","steps":["trace[722257160] 'process raft request'  (duration: 899.698698ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:36:21.394689Z","caller":"traceutil/trace.go:171","msg":"trace[1417706018] linearizableReadLoop","detail":"{readStateIndex:15562; appliedIndex:15562; }","duration":"914.131768ms","start":"2024-10-30T00:36:20.480532Z","end":"2024-10-30T00:36:21.394664Z","steps":["trace[1417706018] 'read index received'  (duration: 914.120796ms)","trace[1417706018] 'applied index is now lower than readState.Index'  (duration: 9.179¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:36:21.566521Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-10-30T00:36:20.480432Z","time spent":"1.074309482s","remote":"127.0.0.1:34042","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:12427 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-10-30T00:36:21.587449Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.106893528s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:36:21.588189Z","caller":"traceutil/trace.go:171","msg":"trace[1704076296] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12511; }","duration":"1.107627979s","start":"2024-10-30T00:36:20.480526Z","end":"2024-10-30T00:36:21.588154Z","steps":["trace[1704076296] 'agreement among raft nodes before linearized reading'  (duration: 1.071228632s)","trace[1704076296] 'range keys from in-memory index tree'  (duration: 35.632892ms)"],"step_count":2}
{"level":"info","ts":"2024-10-30T00:36:21.589651Z","caller":"traceutil/trace.go:171","msg":"trace[1158227405] transaction","detail":"{read_only:false; response_revision:12512; number_of_response:1; }","duration":"205.754972ms","start":"2024-10-30T00:36:21.383880Z","end":"2024-10-30T00:36:21.589635Z","steps":["trace[1158227405] 'process raft request'  (duration: 181.379475ms)","trace[1158227405] 'compare'  (duration: 21.974689ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:36:21.882673Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.232498ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128032896134072882 > lease_revoke:<id:70cc92dad8c2c9dc>","response":"size:29"}
{"level":"info","ts":"2024-10-30T00:36:21.882972Z","caller":"traceutil/trace.go:171","msg":"trace[290432452] linearizableReadLoop","detail":"{readStateIndex:15564; appliedIndex:15563; }","duration":"281.182908ms","start":"2024-10-30T00:36:21.601724Z","end":"2024-10-30T00:36:21.882906Z","steps":["trace[290432452] 'read index received'  (duration: 78.273173ms)","trace[290432452] 'applied index is now lower than readState.Index'  (duration: 202.906849ms)"],"step_count":2}
{"level":"warn","ts":"2024-10-30T00:36:21.883087Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"281.349602ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:36:21.883137Z","caller":"traceutil/trace.go:171","msg":"trace[139745116] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:12512; }","duration":"281.409392ms","start":"2024-10-30T00:36:21.601718Z","end":"2024-10-30T00:36:21.883128Z","steps":["trace[139745116] 'agreement among raft nodes before linearized reading'  (duration: 281.305855ms)"],"step_count":1}
{"level":"warn","ts":"2024-10-30T00:36:21.889952Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"196.757303ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"warn","ts":"2024-10-30T00:36:21.889710Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"286.555526ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-30T00:36:21.890133Z","caller":"traceutil/trace.go:171","msg":"trace[1756724770] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:12515; }","duration":"287.021305ms","start":"2024-10-30T00:36:21.603100Z","end":"2024-10-30T00:36:21.890122Z","steps":["trace[1756724770] 'agreement among raft nodes before linearized reading'  (duration: 285.460865ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:36:21.890670Z","caller":"traceutil/trace.go:171","msg":"trace[356070992] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:12515; }","duration":"196.828466ms","start":"2024-10-30T00:36:21.693181Z","end":"2024-10-30T00:36:21.890009Z","steps":["trace[356070992] 'agreement among raft nodes before linearized reading'  (duration: 195.508297ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:36:22.766094Z","caller":"traceutil/trace.go:171","msg":"trace[461184851] transaction","detail":"{read_only:false; response_revision:12517; number_of_response:1; }","duration":"155.401785ms","start":"2024-10-30T00:36:22.610675Z","end":"2024-10-30T00:36:22.766077Z","steps":["trace[461184851] 'process raft request'  (duration: 155.253869ms)"],"step_count":1}
{"level":"info","ts":"2024-10-30T00:40:49.759164Z","caller":"traceutil/trace.go:171","msg":"trace[339091413] transaction","detail":"{read_only:false; response_revision:12741; number_of_response:1; }","duration":"148.024172ms","start":"2024-10-30T00:40:49.601432Z","end":"2024-10-30T00:40:49.749456Z","steps":["trace[339091413] 'process raft request'  (duration: 147.892238ms)"],"step_count":1}


==> kernel <==
 00:44:12 up 23:01,  0 users,  load average: 0.34, 1.02, 1.27
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [bb09446b146c] <==
W1030 00:35:30.313131       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1030 00:35:30.409726       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1030 00:35:30.409767       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1030 00:35:39.739869       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1030 00:35:39.739489       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1030 00:35:39.739483       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1030 00:35:39.741485       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1030 00:35:39.742012       1 secure_serving.go:213] Serving securely on [::]:8443
I1030 00:35:39.742463       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1030 00:35:39.742913       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1030 00:35:39.751352       1 controller.go:119] Starting legacy_token_tracking_controller
I1030 00:35:39.751396       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1030 00:35:39.757091       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1030 00:35:39.757784       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1030 00:35:39.757839       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1030 00:35:39.758310       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1030 00:35:39.759378       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1030 00:35:39.760333       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1030 00:35:39.760810       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1030 00:35:39.760941       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1030 00:35:39.766605       1 local_available_controller.go:156] Starting LocalAvailability controller
I1030 00:35:39.766671       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1030 00:35:39.767371       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1030 00:35:39.767388       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1030 00:35:39.767563       1 aggregator.go:169] waiting for initial CRD sync...
I1030 00:35:39.803124       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1030 00:35:39.803171       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1030 00:35:39.811480       1 controller.go:78] Starting OpenAPI AggregationController
I1030 00:35:39.812935       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1030 00:35:39.812957       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1030 00:35:39.822834       1 crd_finalizer.go:269] Starting CRDFinalizer
I1030 00:35:39.823062       1 controller.go:142] Starting OpenAPI controller
I1030 00:35:39.823220       1 controller.go:90] Starting OpenAPI V3 controller
I1030 00:35:39.823343       1 naming_controller.go:294] Starting NamingConditionController
I1030 00:35:39.823388       1 establishing_controller.go:81] Starting EstablishingController
I1030 00:35:39.823468       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1030 00:35:39.823613       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1030 00:35:40.317357       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1030 00:35:40.318318       1 aggregator.go:171] initial CRD sync complete...
I1030 00:35:40.318749       1 autoregister_controller.go:144] Starting autoregister controller
I1030 00:35:40.318787       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1030 00:35:40.318798       1 cache.go:39] Caches are synced for autoregister controller
I1030 00:35:40.319440       1 shared_informer.go:320] Caches are synced for configmaps
I1030 00:35:40.320078       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1030 00:35:40.403457       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1030 00:35:40.403467       1 cache.go:39] Caches are synced for LocalAvailability controller
I1030 00:35:40.403730       1 policy_source.go:224] refreshing policies
I1030 00:35:40.404431       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1030 00:35:40.405498       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1030 00:35:40.505000       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1030 00:35:40.505024       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1030 00:35:40.522335       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1030 00:35:40.536807       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1030 00:35:40.611878       1 shared_informer.go:320] Caches are synced for node_authorizer
I1030 00:35:41.516252       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1030 00:35:45.324685       1 controller.go:615] quota admission added evaluator for: endpoints
I1030 00:35:58.909798       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1030 00:35:58.909796       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1030 00:43:33.491047       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1030 00:43:33.600343       1 controller.go:615] quota admission added evaluator for: replicasets.apps


==> kube-apiserver [e94937944de6] <==
W1030 00:33:09.296205       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.296653       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.366276       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.462941       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.464236       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.464468       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.603298       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.603732       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.695593       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.696644       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.698633       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.699757       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.701252       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.701413       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.795785       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.795835       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.796057       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.796155       1 logging.go:55] [core] [Channel #1378 SubChannel #1379]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.796209       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.796904       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.796929       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.796976       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.797491       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.904685       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.995832       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.996040       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:09.996152       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.003082       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.096765       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.096759       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.097863       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.109193       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.195738       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.199485       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.201677       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.303065       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.303375       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.304005       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.304497       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.307599       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.398873       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.496508       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.497461       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.497778       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.497866       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.513953       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.596118       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.596986       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.597125       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.597893       1 logging.go:55] [core] [Channel #18 SubChannel #19]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.728784       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.802310       1 logging.go:55] [core] [Channel #1378 SubChannel #1379]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.902673       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.919054       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:10.926648       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:12.038415       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:12.547452       1 logging.go:55] [core] [Channel #1378 SubChannel #1379]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:12.651779       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:12.688054       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1030 00:33:13.118832       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [49cb8dd1cda4] <==
I1030 00:35:10.434573       1 serving.go:386] Generated self-signed cert in-memory
I1030 00:35:19.180104       1 controllermanager.go:197] "Starting" version="v1.31.0"
I1030 00:35:19.182780       1 controllermanager.go:199] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1030 00:35:19.290121       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1030 00:35:19.290122       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1030 00:35:19.291873       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I1030 00:35:19.292374       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1030 00:35:40.729333       1 controllermanager.go:242] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]poststarthook/start-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-status-local-available-controller ok\\n[+]poststarthook/apiservice-status-remote-available-controller ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\nhealthz check failed\") has prevented the request from succeeding"


==> kube-controller-manager [5bd66c505f3d] <==
I1030 00:35:58.704281       1 shared_informer.go:320] Caches are synced for service account
I1030 00:35:58.704286       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1030 00:35:58.703350       1 shared_informer.go:320] Caches are synced for taint
I1030 00:35:58.704819       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1030 00:35:58.705250       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1030 00:35:58.705297       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1030 00:35:58.705386       1 shared_informer.go:320] Caches are synced for ReplicationController
I1030 00:35:58.705535       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1030 00:35:58.705606       1 shared_informer.go:320] Caches are synced for GC
I1030 00:35:58.703385       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1030 00:35:58.706204       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1030 00:35:58.706455       1 shared_informer.go:320] Caches are synced for node
I1030 00:35:58.707365       1 shared_informer.go:320] Caches are synced for TTL
I1030 00:35:58.707439       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1030 00:35:58.707508       1 shared_informer.go:320] Caches are synced for deployment
I1030 00:35:58.705973       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1030 00:35:58.707996       1 shared_informer.go:320] Caches are synced for ephemeral
I1030 00:35:58.707880       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1030 00:35:58.708168       1 shared_informer.go:320] Caches are synced for expand
I1030 00:35:58.708765       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1030 00:35:58.705270       1 shared_informer.go:320] Caches are synced for PVC protection
I1030 00:35:58.708115       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1030 00:35:58.708550       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1030 00:35:58.708548       1 shared_informer.go:320] Caches are synced for disruption
I1030 00:35:58.709078       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1030 00:35:58.709115       1 shared_informer.go:320] Caches are synced for crt configmap
I1030 00:35:58.709573       1 shared_informer.go:320] Caches are synced for persistent volume
I1030 00:35:58.717728       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1030 00:35:58.717771       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1030 00:35:58.717781       1 shared_informer.go:320] Caches are synced for cidrallocator
I1030 00:35:58.717794       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1030 00:35:58.717939       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1030 00:35:58.719403       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-f8f88784c" duration="313.335¬µs"
I1030 00:35:58.719566       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/users-deployment-65d99cf9bc" duration="79.706¬µs"
I1030 00:35:58.719636       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/flights-deployment-7c55dd7765" duration="39.478¬µs"
I1030 00:35:58.719701       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="154.062¬µs"
I1030 00:35:58.745687       1 shared_informer.go:320] Caches are synced for stateful set
I1030 00:35:58.753561       1 shared_informer.go:320] Caches are synced for attach detach
I1030 00:35:58.758825       1 shared_informer.go:320] Caches are synced for daemon sets
I1030 00:35:58.832179       1 shared_informer.go:320] Caches are synced for job
I1030 00:35:58.849245       1 shared_informer.go:320] Caches are synced for cronjob
I1030 00:35:58.850598       1 shared_informer.go:320] Caches are synced for TTL after finished
I1030 00:35:58.888541       1 shared_informer.go:320] Caches are synced for resource quota
I1030 00:35:58.898251       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1030 00:35:58.901762       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1030 00:35:58.909113       1 shared_informer.go:320] Caches are synced for resource quota
I1030 00:35:59.296087       1 shared_informer.go:320] Caches are synced for garbage collector
I1030 00:35:59.301356       1 shared_informer.go:320] Caches are synced for garbage collector
I1030 00:35:59.301508       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1030 00:36:21.899653       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1030 00:40:26.472806       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-f8f88784c" duration="15.699464ms"
I1030 00:40:31.400670       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-f8f88784c" duration="88.316¬µs"
I1030 00:40:37.385675       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-f8f88784c" duration="564.526¬µs"
I1030 00:40:45.389552       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-f8f88784c" duration="966.408¬µs"
I1030 00:41:28.692031       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1030 00:43:33.726143       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-66bf4c5587" duration="105.362803ms"
I1030 00:43:33.737614       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-66bf4c5587" duration="10.788621ms"
I1030 00:43:33.739110       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-66bf4c5587" duration="1.184388ms"
I1030 00:43:33.830620       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-66bf4c5587" duration="3.276274ms"
I1030 00:43:33.873148       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/airports-deployment-66bf4c5587" duration="69.25¬µs"


==> kube-proxy [89851d6c3ccf] <==
E1030 00:34:59.378775       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1030 00:34:59.466273       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1030 00:34:59.978442       1 server_linux.go:66] "Using iptables proxy"
E1030 00:35:10.597166       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E1030 00:35:11.667341       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E1030 00:35:14.681806       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
E1030 00:35:29.228324       1 server.go:666] "Failed to retrieve node info" err="Get \"https://control-plane.minikube.internal:8443/api/v1/nodes/minikube\": net/http: TLS handshake timeout"
I1030 00:35:41.823776       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1030 00:35:42.017138       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1030 00:35:42.816344       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1030 00:35:42.821115       1 server_linux.go:169] "Using iptables Proxier"
I1030 00:35:42.906861       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1030 00:35:43.016847       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1030 00:35:43.032850       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1030 00:35:43.110155       1 server.go:483] "Version info" version="v1.31.0"
I1030 00:35:43.114161       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1030 00:35:44.607091       1 config.go:104] "Starting endpoint slice config controller"
I1030 00:35:44.606603       1 config.go:197] "Starting service config controller"
I1030 00:35:44.613564       1 config.go:326] "Starting node config controller"
I1030 00:35:44.613852       1 shared_informer.go:313] Waiting for caches to sync for service config
I1030 00:35:44.614800       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1030 00:35:44.615251       1 shared_informer.go:313] Waiting for caches to sync for node config
I1030 00:35:44.914894       1 shared_informer.go:320] Caches are synced for service config
I1030 00:35:44.915373       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1030 00:35:44.915503       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [f3f5c8b9daaf] <==
E1029 19:01:57.582158       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1029 19:01:57.591261       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1029 19:01:57.613178       1 server_linux.go:66] "Using iptables proxy"
I1029 19:01:58.004896       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1029 19:01:58.005084       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1029 19:01:58.042507       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1029 19:01:58.042641       1 server_linux.go:169] "Using iptables Proxier"
I1029 19:01:58.047020       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1029 19:01:58.053756       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1029 19:01:58.061093       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1029 19:01:58.061376       1 server.go:483] "Version info" version="v1.31.0"
I1029 19:01:58.061496       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1029 19:01:58.065340       1 config.go:104] "Starting endpoint slice config controller"
I1029 19:01:58.065371       1 config.go:197] "Starting service config controller"
I1029 19:01:58.065424       1 config.go:326] "Starting node config controller"
I1029 19:01:58.065836       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1029 19:01:58.065842       1 shared_informer.go:313] Waiting for caches to sync for node config
I1029 19:01:58.065850       1 shared_informer.go:313] Waiting for caches to sync for service config
I1029 19:01:58.166597       1 shared_informer.go:320] Caches are synced for service config
I1029 19:01:58.166597       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1029 19:01:58.166648       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [73954bc04967] <==
W1030 00:35:14.665907       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1030 00:35:14.676352       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1030 00:35:14.667129       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1030 00:35:14.676399       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1030 00:35:14.668931       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1030 00:35:14.676428       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1030 00:35:14.669113       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1030 00:35:14.676584       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
E1030 00:35:14.587243       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1030 00:35:14.677902       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1030 00:35:14.677980       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1030 00:35:14.678844       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1030 00:35:14.678980       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1030 00:35:15.077562       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1030 00:35:15.078839       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1030 00:35:15.081145       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1030 00:35:15.081341       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError"
W1030 00:35:28.899354       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1030 00:35:28.902586       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:28.904157       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:28.980359       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:28.980731       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:28.982333       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:28.982601       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
E1030 00:35:28.904157       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:29.308021       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:29.309809       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:29.361836       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:29.362092       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:29.505448       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:29.505851       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:29.808655       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:29.809362       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:30.074800       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:30.075243       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:31.104430       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:31.106302       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:31.113915       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1030 00:35:31.113958       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:31.114757       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:31.114975       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:31.114725       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
E1030 00:35:31.115316       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:31.116102       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1030 00:35:31.116413       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
E1030 00:35:31.116624       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
E1030 00:35:31.116291       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": net/http: TLS handshake timeout" logger="UnhandledError"
W1030 00:35:40.449488       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1030 00:35:40.451825       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1030 00:35:40.503436       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1030 00:35:40.503325       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1030 00:35:40.503535       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1030 00:35:40.503402       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1030 00:35:40.503682       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1030 00:35:40.504854       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1030 00:35:40.504295       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1030 00:35:40.505000       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1030 00:35:40.609403       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1030 00:35:40.609963       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1030 00:35:55.819149       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [aad3d67d9971] <==
I1029 19:01:43.962508       1 serving.go:386] Generated self-signed cert in-memory
W1029 19:01:47.487437       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1029 19:01:47.487569       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1029 19:01:47.487629       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1029 19:01:47.487708       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1029 19:01:47.503435       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1029 19:01:47.503486       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1029 19:01:47.506397       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1029 19:01:47.506491       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1029 19:01:47.506785       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1029 19:01:47.507359       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
W1029 19:01:47.511381       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1029 19:01:47.511458       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1029 19:01:47.511467       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1029 19:01:47.511483       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1029 19:01:47.511532       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1029 19:01:47.511536       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1029 19:01:47.511534       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1029 19:01:47.511543       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1029 19:01:47.511712       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1029 19:01:47.511746       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1029 19:01:47.511746       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1029 19:01:47.511790       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1029 19:01:47.511798       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E1029 19:01:47.511805       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1029 19:01:47.511390       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1029 19:01:47.511765       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E1029 19:01:47.511820       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1029 19:01:47.511537       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1029 19:01:47.511824       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1029 19:01:47.511863       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1029 19:01:47.511882       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1029 19:01:47.511910       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1029 19:01:47.511401       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1029 19:01:47.511961       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1029 19:01:47.511573       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1029 19:01:47.512036       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1029 19:01:47.511836       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1029 19:01:47.512075       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1029 19:01:47.512329       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1029 19:01:47.512357       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1029 19:01:48.452140       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1029 19:01:48.452216       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1029 19:01:48.452147       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1029 19:01:48.452294       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1029 19:01:48.577554       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1029 19:01:48.577597       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1029 19:01:48.579248       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1029 19:01:48.579278       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1029 19:01:48.604648       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1029 19:01:48.604733       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1029 19:01:48.624497       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1029 19:01:48.624548       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1029 19:01:48.657979       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1029 19:01:48.658037       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
I1029 19:01:49.010169       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1030 00:33:03.297902       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1030 00:33:03.298844       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1030 00:33:03.306776       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E1030 00:33:04.897850       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Oct 30 00:40:31 minikube kubelet[2511]: E1030 00:40:31.372803    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:40:37 minikube kubelet[2511]: E1030 00:40:37.371678    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:40:45 minikube kubelet[2511]: E1030 00:40:45.371941    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:40:48 minikube kubelet[2511]: E1030 00:40:48.371794    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:40:58 minikube kubelet[2511]: E1030 00:40:58.371150    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:41:02 minikube kubelet[2511]: E1030 00:41:02.366429    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:41:13 minikube kubelet[2511]: E1030 00:41:13.365808    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:41:13 minikube kubelet[2511]: E1030 00:41:13.365826    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:41:23 minikube kubelet[2511]: E1030 00:41:23.364813    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-volume], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/flights-deployment-7c55dd7765-cpg7h" podUID="7c567a1b-d0c6-46b3-a58b-64c853e24f6c"
Oct 30 00:41:24 minikube kubelet[2511]: E1030 00:41:24.367021    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:41:27 minikube kubelet[2511]: E1030 00:41:27.365886    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:41:33 minikube kubelet[2511]: E1030 00:41:33.362429    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-volume], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/users-deployment-65d99cf9bc-296gl" podUID="b5c5d273-a8e6-469a-b704-b210e014b1ef"
Oct 30 00:41:35 minikube kubelet[2511]: E1030 00:41:35.362339    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-volume], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/users-deployment-65d99cf9bc-jpkjm" podUID="2e86154b-dd2e-4f90-ac5b-ef07d059d5d5"
Oct 30 00:41:36 minikube kubelet[2511]: E1030 00:41:36.363464    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-volume], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/flights-deployment-7c55dd7765-24hg8" podUID="27cbea0e-693c-4bd8-adf8-d9967c0b4254"
Oct 30 00:41:37 minikube kubelet[2511]: E1030 00:41:37.363873    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:41:41 minikube kubelet[2511]: E1030 00:41:41.205819    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/27cbea0e-693c-4bd8-adf8-d9967c0b4254-sqlite-volume podName:27cbea0e-693c-4bd8-adf8-d9967c0b4254 nodeName:}" failed. No retries permitted until 2024-10-30 00:43:43.205091822 +0000 UTC m=+15447.946494868 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/27cbea0e-693c-4bd8-adf8-d9967c0b4254-sqlite-volume") pod "flights-deployment-7c55dd7765-24hg8" (UID: "27cbea0e-693c-4bd8-adf8-d9967c0b4254") : hostPath type check failed: /app/flights.db is not a file
Oct 30 00:41:41 minikube kubelet[2511]: E1030 00:41:41.205966    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/7c567a1b-d0c6-46b3-a58b-64c853e24f6c-sqlite-volume podName:7c567a1b-d0c6-46b3-a58b-64c853e24f6c nodeName:}" failed. No retries permitted until 2024-10-30 00:43:43.205954979 +0000 UTC m=+15447.947358025 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/7c567a1b-d0c6-46b3-a58b-64c853e24f6c-sqlite-volume") pod "flights-deployment-7c55dd7765-cpg7h" (UID: "7c567a1b-d0c6-46b3-a58b-64c853e24f6c") : hostPath type check failed: /app/flights.db is not a file
Oct 30 00:41:41 minikube kubelet[2511]: E1030 00:41:41.363767    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:41:46 minikube kubelet[2511]: E1030 00:41:46.039318    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/b5c5d273-a8e6-469a-b704-b210e014b1ef-sqlite-volume podName:b5c5d273-a8e6-469a-b704-b210e014b1ef nodeName:}" failed. No retries permitted until 2024-10-30 00:43:48.039295719 +0000 UTC m=+15452.780698765 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/b5c5d273-a8e6-469a-b704-b210e014b1ef-sqlite-volume") pod "users-deployment-65d99cf9bc-296gl" (UID: "b5c5d273-a8e6-469a-b704-b210e014b1ef") : hostPath type check failed: /app/users.db is not a file
Oct 30 00:41:46 minikube kubelet[2511]: E1030 00:41:46.140139    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/2e86154b-dd2e-4f90-ac5b-ef07d059d5d5-sqlite-volume podName:2e86154b-dd2e-4f90-ac5b-ef07d059d5d5 nodeName:}" failed. No retries permitted until 2024-10-30 00:43:48.140121003 +0000 UTC m=+15452.881524049 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/2e86154b-dd2e-4f90-ac5b-ef07d059d5d5-sqlite-volume") pod "users-deployment-65d99cf9bc-jpkjm" (UID: "2e86154b-dd2e-4f90-ac5b-ef07d059d5d5") : hostPath type check failed: /app/users.db is not a file
Oct 30 00:41:48 minikube kubelet[2511]: E1030 00:41:48.366645    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:41:56 minikube kubelet[2511]: E1030 00:41:56.366393    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:42:00 minikube kubelet[2511]: E1030 00:42:00.362256    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:42:08 minikube kubelet[2511]: E1030 00:42:08.359758    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:42:12 minikube kubelet[2511]: E1030 00:42:12.361565    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:42:22 minikube kubelet[2511]: E1030 00:42:22.362014    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:42:25 minikube kubelet[2511]: E1030 00:42:25.363988    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:42:35 minikube kubelet[2511]: E1030 00:42:35.357492    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:42:40 minikube kubelet[2511]: E1030 00:42:40.358388    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:42:49 minikube kubelet[2511]: E1030 00:42:49.358379    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:42:52 minikube kubelet[2511]: E1030 00:42:52.620567    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:43:03 minikube kubelet[2511]: E1030 00:43:03.915998    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:43:06 minikube kubelet[2511]: E1030 00:43:06.351782    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:43:15 minikube kubelet[2511]: E1030 00:43:15.353234    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:43:17 minikube kubelet[2511]: E1030 00:43:17.352313    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:43:29 minikube kubelet[2511]: E1030 00:43:29.350859    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:43:32 minikube kubelet[2511]: E1030 00:43:32.350409    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:43:34 minikube kubelet[2511]: I1030 00:43:34.016824    2511 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fnvfz\" (UniqueName: \"kubernetes.io/projected/c3d808b7-428e-44b0-acf1-98ade486170b-kube-api-access-fnvfz\") pod \"airports-deployment-66bf4c5587-cbhw8\" (UID: \"c3d808b7-428e-44b0-acf1-98ade486170b\") " pod="default/airports-deployment-66bf4c5587-cbhw8"
Oct 30 00:43:34 minikube kubelet[2511]: I1030 00:43:34.016929    2511 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sqlite-volume\" (UniqueName: \"kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume\") pod \"airports-deployment-66bf4c5587-cbhw8\" (UID: \"c3d808b7-428e-44b0-acf1-98ade486170b\") " pod="default/airports-deployment-66bf4c5587-cbhw8"
Oct 30 00:43:34 minikube kubelet[2511]: E1030 00:43:34.123030    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume podName:c3d808b7-428e-44b0-acf1-98ade486170b nodeName:}" failed. No retries permitted until 2024-10-30 00:43:34.622469555 +0000 UTC m=+15439.377448719 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume") pod "airports-deployment-66bf4c5587-cbhw8" (UID: "c3d808b7-428e-44b0-acf1-98ade486170b") : hostPath type check failed: /test.db is not a file
Oct 30 00:43:34 minikube kubelet[2511]: E1030 00:43:34.722703    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume podName:c3d808b7-428e-44b0-acf1-98ade486170b nodeName:}" failed. No retries permitted until 2024-10-30 00:43:35.722595075 +0000 UTC m=+15440.477574229 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume") pod "airports-deployment-66bf4c5587-cbhw8" (UID: "c3d808b7-428e-44b0-acf1-98ade486170b") : hostPath type check failed: /test.db is not a file
Oct 30 00:43:35 minikube kubelet[2511]: E1030 00:43:35.729651    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume podName:c3d808b7-428e-44b0-acf1-98ade486170b nodeName:}" failed. No retries permitted until 2024-10-30 00:43:37.729633025 +0000 UTC m=+15442.484612179 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume") pod "airports-deployment-66bf4c5587-cbhw8" (UID: "c3d808b7-428e-44b0-acf1-98ade486170b") : hostPath type check failed: /test.db is not a file
Oct 30 00:43:37 minikube kubelet[2511]: E1030 00:43:37.349735    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-volume], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/flights-deployment-7c55dd7765-cpg7h" podUID="7c567a1b-d0c6-46b3-a58b-64c853e24f6c"
Oct 30 00:43:37 minikube kubelet[2511]: E1030 00:43:37.742636    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume podName:c3d808b7-428e-44b0-acf1-98ade486170b nodeName:}" failed. No retries permitted until 2024-10-30 00:43:41.742612597 +0000 UTC m=+15446.497591751 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume") pod "airports-deployment-66bf4c5587-cbhw8" (UID: "c3d808b7-428e-44b0-acf1-98ade486170b") : hostPath type check failed: /test.db is not a file
Oct 30 00:43:40 minikube kubelet[2511]: E1030 00:43:40.349867    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:43:41 minikube kubelet[2511]: E1030 00:43:41.772430    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume podName:c3d808b7-428e-44b0-acf1-98ade486170b nodeName:}" failed. No retries permitted until 2024-10-30 00:43:49.772413353 +0000 UTC m=+15454.527392507 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume") pod "airports-deployment-66bf4c5587-cbhw8" (UID: "c3d808b7-428e-44b0-acf1-98ade486170b") : hostPath type check failed: /test.db is not a file
Oct 30 00:43:43 minikube kubelet[2511]: E1030 00:43:43.281735    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/7c567a1b-d0c6-46b3-a58b-64c853e24f6c-sqlite-volume podName:7c567a1b-d0c6-46b3-a58b-64c853e24f6c nodeName:}" failed. No retries permitted until 2024-10-30 00:45:45.281717365 +0000 UTC m=+15570.036696519 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/7c567a1b-d0c6-46b3-a58b-64c853e24f6c-sqlite-volume") pod "flights-deployment-7c55dd7765-cpg7h" (UID: "7c567a1b-d0c6-46b3-a58b-64c853e24f6c") : hostPath type check failed: /app/flights.db is not a file
Oct 30 00:43:43 minikube kubelet[2511]: E1030 00:43:43.281774    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/27cbea0e-693c-4bd8-adf8-d9967c0b4254-sqlite-volume podName:27cbea0e-693c-4bd8-adf8-d9967c0b4254 nodeName:}" failed. No retries permitted until 2024-10-30 00:45:45.281768598 +0000 UTC m=+15570.036747752 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/27cbea0e-693c-4bd8-adf8-d9967c0b4254-sqlite-volume") pod "flights-deployment-7c55dd7765-24hg8" (UID: "27cbea0e-693c-4bd8-adf8-d9967c0b4254") : hostPath type check failed: /app/flights.db is not a file
Oct 30 00:43:43 minikube kubelet[2511]: E1030 00:43:43.349834    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:43:48 minikube kubelet[2511]: E1030 00:43:48.110902    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/b5c5d273-a8e6-469a-b704-b210e014b1ef-sqlite-volume podName:b5c5d273-a8e6-469a-b704-b210e014b1ef nodeName:}" failed. No retries permitted until 2024-10-30 00:45:50.110880103 +0000 UTC m=+15574.865859267 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/b5c5d273-a8e6-469a-b704-b210e014b1ef-sqlite-volume") pod "users-deployment-65d99cf9bc-296gl" (UID: "b5c5d273-a8e6-469a-b704-b210e014b1ef") : hostPath type check failed: /app/users.db is not a file
Oct 30 00:43:48 minikube kubelet[2511]: E1030 00:43:48.210954    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/2e86154b-dd2e-4f90-ac5b-ef07d059d5d5-sqlite-volume podName:2e86154b-dd2e-4f90-ac5b-ef07d059d5d5 nodeName:}" failed. No retries permitted until 2024-10-30 00:45:50.21093514 +0000 UTC m=+15574.965914294 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/2e86154b-dd2e-4f90-ac5b-ef07d059d5d5-sqlite-volume") pod "users-deployment-65d99cf9bc-jpkjm" (UID: "2e86154b-dd2e-4f90-ac5b-ef07d059d5d5") : hostPath type check failed: /app/users.db is not a file
Oct 30 00:43:48 minikube kubelet[2511]: E1030 00:43:48.349303    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-volume], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/users-deployment-65d99cf9bc-296gl" podUID="b5c5d273-a8e6-469a-b704-b210e014b1ef"
Oct 30 00:43:49 minikube kubelet[2511]: E1030 00:43:49.821293    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume podName:c3d808b7-428e-44b0-acf1-98ade486170b nodeName:}" failed. No retries permitted until 2024-10-30 00:44:05.821276702 +0000 UTC m=+15470.576255856 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume") pod "airports-deployment-66bf4c5587-cbhw8" (UID: "c3d808b7-428e-44b0-acf1-98ade486170b") : hostPath type check failed: /test.db is not a file
Oct 30 00:43:51 minikube kubelet[2511]: E1030 00:43:51.350326    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-volume], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/users-deployment-65d99cf9bc-jpkjm" podUID="2e86154b-dd2e-4f90-ac5b-ef07d059d5d5"
Oct 30 00:43:54 minikube kubelet[2511]: E1030 00:43:54.350088    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-volume], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/flights-deployment-7c55dd7765-24hg8" podUID="27cbea0e-693c-4bd8-adf8-d9967c0b4254"
Oct 30 00:43:55 minikube kubelet[2511]: E1030 00:43:55.350273    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"
Oct 30 00:43:55 minikube kubelet[2511]: E1030 00:43:55.351495    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:44:05 minikube kubelet[2511]: E1030 00:44:05.819576    2511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume podName:c3d808b7-428e-44b0-acf1-98ade486170b nodeName:}" failed. No retries permitted until 2024-10-30 00:44:37.819555854 +0000 UTC m=+15502.577258430 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "sqlite-volume" (UniqueName: "kubernetes.io/host-path/c3d808b7-428e-44b0-acf1-98ade486170b-sqlite-volume") pod "airports-deployment-66bf4c5587-cbhw8" (UID: "c3d808b7-428e-44b0-acf1-98ade486170b") : hostPath type check failed: /test.db is not a file
Oct 30 00:44:08 minikube kubelet[2511]: E1030 00:44:08.347387    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-nc944" podUID="5315e1f2-cde3-4af7-9b08-e9638bff955f"
Oct 30 00:44:08 minikube kubelet[2511]: E1030 00:44:08.347592    2511 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"airports\" with ImagePullBackOff: \"Back-off pulling image \\\"asa_container\\\"\"" pod="default/airports-deployment-f8f88784c-p8g9q" podUID="51ea31bd-86a5-4808-8f66-6dba6638fcf7"


==> storage-provisioner [4eda9a39d6e9] <==
I1030 00:36:00.793615       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1030 00:36:00.834018       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1030 00:36:00.835355       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1030 00:36:21.686738       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1030 00:36:21.693311       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_33f299da-b659-49a0-a85d-ae72350b0142!
I1030 00:36:21.783811       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"3b56871c-293b-4e21-bc8e-63ca38d838ea", APIVersion:"v1", ResourceVersion:"12511", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_33f299da-b659-49a0-a85d-ae72350b0142 became leader
I1030 00:36:21.981643       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_33f299da-b659-49a0-a85d-ae72350b0142!


==> storage-provisioner [b891832b6ea3] <==
I1030 00:35:19.671740       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1030 00:35:29.672453       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": net/http: TLS handshake timeout

